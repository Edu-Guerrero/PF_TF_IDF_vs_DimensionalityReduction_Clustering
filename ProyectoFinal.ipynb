{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZiW80gL-FxZ",
        "outputId": "83bc7d19-ac27-4256-c0e0-30eba81b3aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to documentos\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_document(zip_path, extract_to='documentos'):\n",
        "    # Create the target directory if it doesn't exist\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"Files extracted to {extract_to}\")\n",
        "\n",
        "# Example usage\n",
        "zip_path = 'Documentos.zip'  # Replace with your zip file path\n",
        "unzip_document(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: do a pip install unicode\n",
        "\n",
        "!pip install unidecode\n",
        "!python -m spacy download es_core_news_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyyB0jWLCkPf",
        "outputId": "57581e32-ca5f-420a-dae3-e066947e194a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Collecting es-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.7.0/es_core_news_lg-3.7.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: delete the fder documentos_procesados\n",
        "\n",
        "import shutil\n",
        "\n",
        "def delete_directory(dir_path):\n",
        "    try:\n",
        "        shutil.rmtree(dir_path)\n",
        "        print(f\"Directory '{dir_path}' deleted successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Directory '{dir_path}' not found.\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting directory '{dir_path}': {e}\")\n",
        "\n",
        "# Example usage\n",
        "delete_directory('documentos_procesados')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvX9TA14JdxK",
        "outputId": "1c8dbdaf-03a0-4167-f741-06f0f20c93dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'documentos_procesados' not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize spaCy with Spanish model\n",
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "# Initialize Spanish stopwords\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Define source and destination folders\n",
        "carpeta_origen = 'documentos'\n",
        "carpeta_destino = 'documentos_procesados'\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(carpeta_destino, exist_ok=True)\n",
        "\n",
        "def preprocesar_nombre_archivo(nombre_archivo):\n",
        "    # Split the filename by '_'\n",
        "    partes = nombre_archivo.split('_')\n",
        "\n",
        "    # Process each part\n",
        "    partes_procesadas = []\n",
        "    for parte in partes:\n",
        "        # Remove non-alphanumeric characters\n",
        "        parte = re.sub(r'\\W+', ' ', parte)\n",
        "\n",
        "        # Convert to lowercase and remove accents\n",
        "        parte = unidecode(parte.lower())\n",
        "\n",
        "        # Process with spaCy\n",
        "        doc = nlp(parte)\n",
        "\n",
        "        # Filter stopwords and get lemmas\n",
        "        palabras = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "\n",
        "        # Join processed words for this part\n",
        "        parte_procesado = '_'.join(palabras)\n",
        "\n",
        "        if parte_procesado:\n",
        "            partes_procesadas.append(parte_procesado)\n",
        "\n",
        "    # Join all processed parts\n",
        "    nombre_procesado = '_'.join(partes_procesadas)\n",
        "\n",
        "    return nombre_procesado\n",
        "\n",
        "def preprocesar_texto(texto):\n",
        "    # Convert to lowercase and remove accents\n",
        "    texto = unidecode(texto.lower())\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    texto = re.sub(r'\\W+', ' ', texto)\n",
        "\n",
        "    # Process with spaCy\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Filter words and get lemmas\n",
        "    palabras = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "\n",
        "    # Join processed words\n",
        "    texto_procesado = ' '.join(palabras)\n",
        "\n",
        "    return texto_procesado\n",
        "\n",
        "def procesar_archivos(carpeta_origen, carpeta_destino):\n",
        "    for archivo in os.listdir(carpeta_origen):\n",
        "        if archivo.endswith('.txt'):\n",
        "            ruta_origen = os.path.join(carpeta_origen, archivo)\n",
        "\n",
        "            # Preprocess filename\n",
        "            nombre_procesado = preprocesar_nombre_archivo(archivo)\n",
        "            ruta_destino = os.path.join(carpeta_destino, f\"{archivo}.txt\")\n",
        "\n",
        "            try:\n",
        "                # Read original file\n",
        "                with open(ruta_origen, 'r', encoding='utf-8') as f:\n",
        "                    contenido = f.read()\n",
        "\n",
        "                # Process file content\n",
        "                contenido_procesado = preprocesar_texto(contenido)\n",
        "\n",
        "                # Save processed content with processed filename\n",
        "                with open(ruta_destino, 'w', encoding='utf-8') as f:\n",
        "                    f.write(contenido_procesado)\n",
        "\n",
        "                print(f\"Processed: {archivo} -> {nombre_procesado}.txt\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {archivo}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    procesar_archivos(carpeta_origen, carpeta_destino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG3Opo4gCRsI",
        "outputId": "bda1429e-ee51-42d2-eacf-eb29c3c50a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: 68_Enologia_y_Cocteleria.txt -> enologio_cocteleria_txt.txt\n",
            "Processed: 186_Sistemas_Lean.txt -> sistema_lean_txt.txt\n",
            "Processed: 56_Carniceria.txt -> carniceria_txt.txt\n",
            "Processed: 6_Ingles_Nivel_1.txt -> ingl_nivel_txt.txt\n",
            "Processed: 113_Negocios_Internacionales.txt -> negocio_internacional_txt.txt\n",
            "Processed: 66_Reposteria_y_Chocolateria.txt -> reposteria_chocolaterio_txt.txt\n",
            "Processed: 133_Taller_de_Arte_2.txt -> taller_arte_txt.txt\n",
            "Processed: 46_Matematica_Empresarial_+Ej.txt -> matematico_empresarial_ej_txt.txt\n",
            "Processed: 100_Principios_de_Seguros.txt -> principio_seguro_txt.txt\n",
            "Processed: 82_Metodologias_de_Ensenanza.txt -> metodologia_ensenanza_txt.txt\n",
            "Processed: 130_Arte_y_Contexto_Social.txt -> arte_contexto_social_txt.txt\n",
            "Processed: 13_Ingles_Nivel_4.txt -> ingl_nivel_txt.txt\n",
            "Processed: 84_Ensenanza_de_Ciencias_Sociales.txt -> ensenanza_ciencia_social_txt.txt\n",
            "Processed: 7_Ingles_Nivel_2.txt -> ingl_nivel_txt.txt\n",
            "Processed: 97_Principios_de_Marketing.txt -> principio_marketing_txt.txt\n",
            "Processed: 151_Ecuaciones_Diferenciales.txt -> ecuación_diferencial_txt.txt\n",
            "Processed: 122_Fotografia_1.txt -> fotografia_txt.txt\n",
            "Processed: 26_Electronica_Basica_+Lab.txt -> electronico_basico_lab_txt.txt\n",
            "Processed: 115_Tributacion_y_Entorno_Legal.txt -> tributacion_entorno_legal_txt.txt\n",
            "Processed: 148_Calculo_para_Ciencias_1.txt -> calculo_ciencia_txt.txt\n",
            "Processed: 135_Gestion_y_Produccion_Cultural.txt -> gestion_produccion_cultural_txt.txt\n",
            "Processed: 18_Calculo_Vectorial.txt -> calculo_vectorial_txt.txt\n",
            "Processed: 164_Analisis_Funcional.txt -> analisis_funcional_txt.txt\n",
            "Processed: 62_Pasteleria.txt -> pasteleria_txt.txt\n",
            "Processed: 127_Laboratorio_de_Creacion_2.txt -> laboratorio_creacion_txt.txt\n",
            "Processed: 91_Inclusion_y_Diversidad.txt -> inclusion_diversidad_txt.txt\n",
            "Processed: 38_Sistemas_Operativos.txt -> sistema_operativo_txt.txt\n",
            "Processed: 67_Innovacion_Culinaria.txt -> innovacion_culinario_txt.txt\n",
            "Processed: 4_Quimica_General_1_+Lab_Ej.txt -> quimico_general_lab_ej_txt.txt\n",
            "Processed: 185_Ingenieria_de_la_Calidad_+_Lab.txt -> ingenieria_calidad_lab_txt.txt\n",
            "Processed: 109_Investigacion_de_Mercados.txt -> investigacion_mercado_txt.txt\n",
            "Processed: 40_Practica_Pre-Profesional_PASEM.txt -> practicar_pre_profesional_pasem_txt.txt\n",
            "Processed: 47_Conceptos_y_Tecnicas_1.txt -> concepto_tecnica_txt.txt\n",
            "Processed: 50_Nutricion_Humana_+Lab.txt -> nutricion_humano_lab_txt.txt\n",
            "Processed: 142_Produccion_&_Exhibicion.txt -> produccion_exhibicion_txt.txt\n",
            "Processed: 118_Fundamentos_de_las_Artes.txt -> fundamento_art_txt.txt\n",
            "Processed: 27_Estructuras_de_Datos.txt -> estructura_dato_txt.txt\n",
            "Processed: 42_Seguridad_Informatica.txt -> seguridad_informatico_txt.txt\n",
            "Processed: 171_Barra_para_danza_contemporanea.txt -> barra_danza_contemporaneo_txt.txt\n",
            "Processed: 162_Algebra_Abstracta_1.txt -> algebra_abstracto_txt.txt\n",
            "Processed: 117_Herramientas_Digitales_1.txt -> herramienta_digital_txt.txt\n",
            "Processed: 43_Mineria_de_Datos.txt -> mineria_dato_txt.txt\n",
            "Processed: 3_Calculo_Diferencial_+_Ej.txt -> calculo_diferencial_ej_txt.txt\n",
            "Processed: 30_Programacion_Avanzada_de_Apps.txt -> programacion_avanzado_app_txt.txt\n",
            "Processed: 10_Autoconocimiento.txt -> autoconocimiento_txt.txt\n",
            "Processed: 44_Aplicaciones_Distribuidas.txt -> aplicación_distribuido_txt.txt\n",
            "Processed: 112_Gestion_del_Talento.txt -> gestion_talento_txt.txt\n",
            "Processed: 114_Innovacion_y_Sustentabilidad.txt -> innovacion_sustentabilidad_txt.txt\n",
            "Processed: 83_Ensenanza_de_Matematicas.txt -> ensenanza_matematica_txt.txt\n",
            "Processed: 12_Ingles_Nivel_3.txt -> ingl_nivel_txt.txt\n",
            "Processed: 147_Modelado_3D_1.txt -> modelado_txt.txt\n",
            "Processed: 77_Neurociencia_y_Educacion.txt -> neurociencia_educacion_txt.txt\n",
            "Processed: 51_Introduccion_a_la_Hospitalidad.txt -> introduccion_hospitalidad_txt.txt\n",
            "Processed: 163_Geometria_Diferencial.txt -> geometria_diferencial_txt.txt\n",
            "Processed: 169_Improvisacion.txt -> improvisacion_txt.txt\n",
            "Processed: 106_Negociacion.txt -> negociacion_txt.txt\n",
            "Processed: 149_Teoria_de_Grupos.txt -> teoria_grupo_txt.txt\n",
            "Processed: 92_Proyecto_Integrador_EDU.txt -> proyecto_integrador_edu_txt.txt\n",
            "Processed: 54_Practica_Culinaria_1.txt -> practicar_culinario_txt.txt\n",
            "Processed: 79_Coloquios_EDU.txt -> coloquio_edu_txt.txt\n",
            "Processed: 99_Gerencia_de_Costos.txt -> gerencia_costo_txt.txt\n",
            "Processed: 41_Coloquios_ING.txt -> coloquio_ing_txt.txt\n",
            "Processed: 131_Taller_de_Investigacion.txt -> taller_investigacion_txt.txt\n",
            "Processed: 93_Practica_4.txt -> practicar_txt.txt\n",
            "Processed: 138_Enfasis_3.txt -> enfasis_txt.txt\n",
            "Processed: 104_Finanzas_Corporativas.txt -> finanza_corporativo_txt.txt\n",
            "Processed: 15_Matematicas_Discretas.txt -> matematica_discreto_txt.txt\n",
            "Processed: 55_Administracion_de_A_&_B.txt -> administracion_b_txt.txt\n",
            "Processed: 69_Administracion_de_Eventos.txt -> administracion_evento_txt.txt\n",
            "Processed: 125_Nuevos_Medios.txt -> nuevo_medio_txt.txt\n",
            "Processed: 160_Ecuaciones_Diferenciales_Parciales.txt -> ecuación_diferencial_parcial_txt.txt\n",
            "Processed: 87_Practica_2.txt -> practicar_txt.txt\n",
            "Processed: 2_Taller_de_Ing._Cs._Computacion.txt -> taller_ing_cs_computacion_txt.txt\n",
            "Processed: 166_Topologia_1.txt -> topologia_txt.txt\n",
            "Processed: 16_Introduccion_a_la_Economia.txt -> introduccion_economia_txt.txt\n",
            "Processed: 5_Cosmos.txt -> cosmo_txt.txt\n",
            "Processed: 20_Ingles_Nivel_6.txt -> ingl_nivel_txt.txt\n",
            "Processed: 73_Teorias_del_Aprendizaje.txt -> teoria_aprendizaje_txt.txt\n",
            "Processed: 34_Inteligencia_Artificial.txt -> inteligencia_artificial_txt.txt\n",
            "Processed: 143_Programacion_Para_Diseño_1.txt -> programacion_diseno_txt.txt\n",
            "Processed: 19_Ingles_Nivel_5.txt -> ingl_nivel_txt.txt\n",
            "Processed: 103_Coloquios_adm.txt -> coloquio_adm_txt.txt\n",
            "Processed: 129_Coloquios_ART.txt -> coloquio_art_txt.txt\n",
            "Processed: 86_Bilingualism.txt -> bilingualism_txt.txt\n",
            "Processed: 14_Programacion_Avanzada_en_C++.txt -> programacion_avanzado_c_txt.txt\n",
            "Processed: 70_Gerencia_Financiera_HSP.txt -> gerencia_financiero_hsp_txt.txt\n",
            "Processed: 137_Matematicas_Cotidianas.txt -> matematica_cotidiano_txt.txt\n",
            "Processed: 120_Laboratorio_de_Creacion_1.txt -> laboratorio_creacion_txt.txt\n",
            "Processed: 153_Introduccion_a_Probabilidades.txt -> introduccion_probabilidad_txt.txt\n",
            "Processed: 60_Coloquios_Gastr.txt -> coloquio_gastr_txt.txt\n",
            "Processed: 23_Fisica_para_Ingenieria_2_+Lab_Ej.txt -> fisico_ingenieria_lab_ej_txt.txt\n",
            "Processed: 140_Arte_y_Educacion___Curaduria.txt -> arte_educacion_curadurio_txt.txt\n",
            "Processed: 123_Fundamentos_de_Escultura.txt -> fundamento_escultura_txt.txt\n",
            "Processed: 165_Algebra_Abstracta_2.txt -> algebra_abstracto_txt.txt\n",
            "Processed: 81_Practica_1.txt -> practicar_txt.txt\n",
            "Processed: 156_Inferencia_Estadistica.txt -> inferencia_estadisticar_txt.txt\n",
            "Processed: 35_Base_de_Datos.txt -> base_dato_txt.txt\n",
            "Processed: 154_Fundamentos_de_Geometria.txt -> fundamento_geometria_txt.txt\n",
            "Processed: 176_Lenguaje_Visual_y_Montaje.txt -> lenguaje_visual_montaje_txt.txt\n",
            "Processed: 53_Conceptos_y_Tecnicas_2_+PRA.txt -> concepto_tecnica_pra_txt.txt\n",
            "Processed: 29_Teoria_de_la_Computacion.txt -> teoria_computacion_txt.txt\n",
            "Processed: 128_Taller_de_Arte_1.txt -> taller_arte_txt.txt\n",
            "Processed: 144_Programacion_Para_Diseño_2.txt -> programacion_diseno_txt.txt\n",
            "Processed: 179_Storytelling.txt -> storytelling_txt.txt\n",
            "Processed: 96_Evolucion.txt -> evolucion_txt.txt\n",
            "Processed: 49_Panaderia.txt -> panaderia_txt.txt\n",
            "Processed: 134_Enfasis_2.txt -> enfasis_txt.txt\n",
            "Processed: 90_Practica_3.txt -> practicar_txt.txt\n",
            "Processed: 145_Programacion_Para_Diseño_3.txt -> programacion_diseno_txt.txt\n",
            "Processed: 52_Contabilidad_Empresarial.txt -> contabilidad_empresarial_txt.txt\n",
            "Processed: 48_Principios_de_Administracion.txt -> principio_administracion_txt.txt\n",
            "Processed: 17_Fisica_para_Ingenieria_1_+Lab_Ej.txt -> fisico_ingenieria_lab_ej_txt.txt\n",
            "Processed: 168_Analisis_Real.txt -> analisis_real_txt.txt\n",
            "Processed: 152_Calculo_para_Ciencias_2.txt -> calculo_ciencia_txt.txt\n",
            "Processed: 98_Estadistica_Empresarial_+Lab.txt -> estadisticar_empresarial_lab_txt.txt\n",
            "Processed: 24_Aprendizaje_y_Servicio_PASEC.txt -> aprendizaje_servicio_pasec_txt.txt\n",
            "Processed: 57_Alta_Cocina_Ecuatoriana.txt -> alto_cocina_ecuatoriano_txt.txt\n",
            "Processed: 132_Enfasis_1.txt -> enfasis_txt.txt\n",
            "Processed: 184_Ergonomia.txt -> ergonomia_txt.txt\n",
            "Processed: 74_Desarrollo__Niño_y_Adolescente.txt -> desarrollo_nino_adolescente_txt.txt\n",
            "Processed: 89_Ensenanza_Integrada_de_CITIAM.txt -> ensenanza_integrado_citiam_txt.txt\n",
            "Processed: 36_Aprendizaje_Automatico.txt -> aprendizaje_automatico_txt.txt\n",
            "Processed: 141_Temas_en_Comunicacion_y_Arte.txt -> tema_comunicacion_arte_txt.txt\n",
            "Processed: 11_Ser_y_Cosmos.txt -> ser_cosmo_txt.txt\n",
            "Processed: 167_Topologia_2.txt -> topologia_txt.txt\n",
            "Processed: 180_Diseño_de_Produccion.txt -> diseno_produccion_txt.txt\n",
            "Processed: 139_Taller_de_Arte_3.txt -> taller_arte_txt.txt\n",
            "Processed: 63_Servicio_y_Protocolo.txt -> servicio_protocolo_txt.txt\n",
            "Processed: 94_Zoologia_+Lab.txt -> zoologia_lab_txt.txt\n",
            "Processed: 175_Lenguaje_del_Cine.txt -> lenguaje_cine_txt.txt\n",
            "Processed: 121_Dibujo_para_Arte_y_Diseno.txt -> dibujo_arte_diseno_txt.txt\n",
            "Processed: 95_Fisiologia_+Lab.txt -> fisiologia_lab_txt.txt\n",
            "Processed: 33_Diseño_de_Sistemas.txt -> diseno_sistema_txt.txt\n",
            "Processed: 1_Escritura_Academica.txt -> escritura_academica_txt.txt\n",
            "Processed: 177_Cinematografia.txt -> cinematografia_txt.txt\n",
            "Processed: 37_Redes_+Lab.txt -> red_lab_txt.txt\n",
            "Processed: 71_Identidad_Culinaria.txt -> identidad_culinario_txt.txt\n",
            "Processed: 32_Emprendimiento.txt -> emprendimiento_txt.txt\n",
            "Processed: 183_Procesos,_Metodos_y_Estandares.txt -> proceso_metodo_estandar_txt.txt\n",
            "Processed: 110_Creatividad_Empresarial.txt -> creatividad_empresarial_txt.txt\n",
            "Processed: 61_Alta_Cocina_Francesa.txt -> alto_cocina_francés_txt.txt\n",
            "Processed: 9_Calculo_Integral_+_Ej.txt -> calculo_integral_ej_txt.txt\n",
            "Processed: 181_Inv._de_Operaciones_1_+Lab.txt -> inv_operación_lab_txt.txt\n",
            "Processed: 159_Combinatoria_y_Grafos.txt -> combinatorio_grafo_txt.txt\n",
            "Processed: 85_Arte_y_Educacion.txt -> arte_educacion_txt.txt\n",
            "Processed: 45_Proyecto_Integrador_CMP.txt -> proyecto_integrador_cmp_txt.txt\n",
            "Processed: 136_Laboratorio_de_Creacion_3.txt -> laboratorio_creacion_txt.txt\n",
            "Processed: 76_Estadistica_para_CCSS.txt -> estadisticar_ccss_txt.txt\n",
            "Processed: 25_Cultura_Gastronomica.txt -> cultura_gastronomico_txt.txt\n",
            "Processed: 119_Composicion_Visual_1.txt -> composicion_visual_txt.txt\n",
            "Processed: 107_Operaciones_+Lab.txt -> operación_lab_txt.txt\n",
            "Processed: 28_Probabilidad_y_Estadistica_+Ej.txt -> probabilidad_estadisticar_ej_txt.txt\n",
            "Processed: 58_Introduccion_a_la_Biologia_+Ej.txt -> introduccion_biologia_ej_txt.txt\n",
            "Processed: 59_Introduccion_al_Marketing_HSP.txt -> introduccion_marketing_hsp_txt.txt\n",
            "Processed: 155_Logica_y_Teoria_de_Conjuntos.txt -> logico_teoria_conjunto_txt.txt\n",
            "Processed: 116_Analisis_Estrategico_ADM.txt -> analisis_estrategico_adm_txt.txt\n",
            "Processed: 172_Composicion.txt -> composicion_txt.txt\n",
            "Processed: 64_Practica_Culinaria_2.txt -> practicar_culinario_txt.txt\n",
            "Processed: 126_Arte_Contemporaneo.txt -> arte_contemporaneo_txt.txt\n",
            "Processed: 31_Organizacion_de_Computadores.txt -> organizacion_computador_txt.txt\n",
            "Processed: 8_Programacion_en_C++_+Ej.txt -> programacion_c_ej_txt.txt\n",
            "Processed: 80_Ensenanza_de_Lenguaje.txt -> ensenanza_lenguaje_txt.txt\n",
            "Processed: 170_Danza_Moderna_2.txt -> danza_moderno_txt.txt\n",
            "Processed: 173_Performance.txt -> performance_txt.txt\n",
            "Processed: 178_Sonido.txt -> sonido_txt.txt\n",
            "Processed: 174_Proyecto_final_en_Danza.txt -> proyecto_final_danza_txt.txt\n",
            "Processed: 72_Fundamentos_de_la_Educacion.txt -> fundamento_educacion_txt.txt\n",
            "Processed: 124_Teoria_Critica_1__Arte_&_Media.txt -> teoria_criticar_arte_medio_txt.txt\n",
            "Processed: 88_Ensenanza_de_Ciencias.txt -> ensenanza_ciencia_txt.txt\n",
            "Processed: 21_Programacion_de_Apps.txt -> programacion_app_txt.txt\n",
            "Processed: 78_Planificacion_y_Evaluacion_2.txt -> planificacion_evaluacion_txt.txt\n",
            "Processed: 75_Planificacion_y_Evaluacion_1.txt -> planificacion_evaluacion_txt.txt\n",
            "Processed: 158_Teoria_de_Numeros.txt -> teoria_numero_txt.txt\n",
            "Processed: 108_Economia_y_Negocios.txt -> economia_negocio_txt.txt\n",
            "Processed: 146_Juegos_y_Narrativa.txt -> juego_narrativa_txt.txt\n",
            "Processed: 150_Variable_Compleja.txt -> variable_complejo_txt.txt\n",
            "Processed: 22_Algebra_Lineal_1_+Ej.txt -> algebra_lineal_ej_txt.txt\n",
            "Processed: 65_Alta_Cocina_Mundial.txt -> alto_cocina_mundial_txt.txt\n",
            "Processed: 102_Principios_de_Finanzas.txt -> principio_finanza_txt.txt\n",
            "Processed: 161_Algebra_Lineal_2.txt -> algebra_lineal_txt.txt\n",
            "Processed: 39_Proyectos__Gerencia_y_Analisis.txt -> proyecto_gerencia_analisis_txt.txt\n",
            "Processed: 101_Analisis_de_Datos.txt -> analisis_dato_txt.txt\n",
            "Processed: 111_Proyectos_Empresariales.txt -> proyecto_empresarial_txt.txt\n",
            "Processed: 182_Control_de_Produccion.txt -> control_produccion_txt.txt\n",
            "Processed: 157_Analisis_Numerico.txt -> analisis_numerico_txt.txt\n",
            "Processed: 105_Marketing_Digital.txt -> marketing_digital_txt.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib  # Para guardar el vectorizador\n",
        "\n",
        "def calculate_tf_idf(folder_path, output_file, vectorizer_file):\n",
        "    \"\"\"\n",
        "    Calculates TF-IDF vectors for documents in a folder, saves them to a CSV file,\n",
        "    and saves the TF-IDF vectorizer as a .joblib file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Store filenames and text contents\n",
        "    filenames = []\n",
        "    documents = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "                    filenames.append(filename)\n",
        "                    documents.append(text)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "    # Fit and transform the documents\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Save the TF-IDF vectorizer to a .joblib file\n",
        "    joblib.dump(vectorizer, vectorizer_file)\n",
        "    print(f\"TF-IDF vectorizer saved to {vectorizer_file}\")\n",
        "\n",
        "    # Create a DataFrame from the TF-IDF matrix\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Insert filenames as the first column\n",
        "    tfidf_df.insert(0, 'filename', filenames)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    tfidf_df.to_csv(output_file, index=False)\n",
        "    print(f\"TF-IDF vectors saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "folder_path = 'documentos_procesados'\n",
        "output_file = 'tf_idf_documentos.csv'\n",
        "vectorizer_file = 'tfidf_vectorizer.joblib'\n",
        "calculate_tf_idf(folder_path, output_file, vectorizer_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUvrXK4TECat",
        "outputId": "44a90f6f-8ecd-419d-80aa-c4db838bb63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vectorizer saved to tfidf_vectorizer.joblib\n",
            "TF-IDF vectors saved to tf_idf_documentos.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import joblib  # Para guardar el vectorizador\n",
        "\n",
        "def calculate_log_tf_idf(folder_path, output_file, vectorizer_file):\n",
        "    \"\"\"\n",
        "    Calculates logarithmic TF and standard IDF vectors for documents in a folder,\n",
        "    and saves the CountVectorizer as a .joblib file.\n",
        "    \"\"\"\n",
        "    # Store filenames and text contents\n",
        "    filenames = []\n",
        "    documents = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "                    filenames.append(filename)\n",
        "                    documents.append(text)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "    # Create count vectorizer\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    term_freq_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Save the CountVectorizer to a .joblib file\n",
        "    joblib.dump(count_vectorizer, vectorizer_file)\n",
        "    print(f\"CountVectorizer saved to {vectorizer_file}\")\n",
        "\n",
        "    # Get feature names\n",
        "    feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Calculate logarithmic TF\n",
        "    log_tf_matrix = np.log1p(term_freq_matrix.toarray())\n",
        "\n",
        "    # Calculate IDF\n",
        "    doc_count = len(documents)\n",
        "    idf_vector = np.log(doc_count / (np.sum(term_freq_matrix.toarray() > 0, axis=0) + 1))\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tfidf_matrix = log_tf_matrix * idf_vector\n",
        "\n",
        "    # Create DataFrame\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix, columns=feature_names)\n",
        "    tfidf_df.insert(0, 'filename', filenames)\n",
        "\n",
        "    # Save to CSV\n",
        "    tfidf_df.to_csv(output_file, index=False)\n",
        "    print(f\"Logarithmic TF-IDF vectors saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "folder_path = 'documentos_procesados'\n",
        "output_file = 'tf_idf_documentos_2.csv'\n",
        "vectorizer_file = 'count_vectorizer.joblib'\n",
        "calculate_log_tf_idf(folder_path, output_file, vectorizer_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFDJYKlqQ0M8",
        "outputId": "dccc660f-4c50-4772-9f7a-e9677245c6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer saved to count_vectorizer.joblib\n",
            "Logarithmic TF-IDF vectors saved to tf_idf_documentos_2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"curso de programación, diseño gráfico y cocina internacional con técnicas avanzadas de cocina y recetas modernas\",\n",
        "    \"Curso de escritura creativa y técnica para estudiantes de música interesados en mejorar sus habilidades de composición\",\n",
        "    \"curso de idiomas inicial para estudiantes que quieren aprender inglés de manera interactiva y efectiva con enfoque en conversación\",\n",
        "    \"curso sobre inteligencia artificial, tecnología de vanguardia, innovación disruptiva y desarrollo de aplicaciones inteligentes en el mercado\",\n",
        "    \"curso para estudios de matemáticas aplicadas y artes liberales, combinando teoría matemática avanzada con análisis crítico cultural\",\n",
        "]\n",
        "\n",
        "queries_procesadas = []\n",
        "\n",
        "for query in queries:\n",
        "    # Convert to lowercase and remove accents\n",
        "    query_pre = unidecode(query.lower())\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    query_pre = re.sub(r'\\W+', ' ', query_pre)\n",
        "\n",
        "    # Process with spaCy\n",
        "    doc = nlp(query_pre)\n",
        "\n",
        "    # Filter words and get lemmas\n",
        "    palabras = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "\n",
        "    # Join processed words\n",
        "    query_preprocesada = ' '.join(palabras)\n",
        "\n",
        "    queries_procesadas.append(query_preprocesada)\n",
        "\n",
        "for query in queries_procesadas:\n",
        "    print(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuqdoBYczc34",
        "outputId": "cc9951a9-0a2d-4885-b116-3c2faaeafa05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curso programacion diseno grafico cocina internacional tecnica avanzado cocina receta moderno\n",
            "curso escritura creativo tecnico estudiante musica interesado mejorar habilidad composicion\n",
            "curso idioma inicial estudiante querer aprender ingl manera interactivo efectivo enfoque conversacion\n",
            "curso inteligencia artificial tecnologia vanguardia innovacion disruptivo desarrollo aplicación inteligente mercado\n",
            "curso estudio matematica aplicado art liberal combinar teoria matematico avanzado analisis critico cultural\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargar el vectorizador entrenado (asegúrate de haberlo guardado previamente)\n",
        "import joblib\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
        "\n",
        "queries_vector = []\n",
        "\n",
        "for query in queries_procesadas:\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "    print(query_vector)\n",
        "    queries_vector.append(query_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTzDDd80X7J",
        "outputId": "a6ee7024-fd96-48db-84ef-f9ae3c65ea56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 107)\t0.20077436589628997\n",
            "  (0, 180)\t0.5439117445998276\n",
            "  (0, 310)\t0.08069243018616552\n",
            "  (0, 371)\t0.24995123897947033\n",
            "  (0, 589)\t0.3641364022100168\n",
            "  (0, 688)\t0.3437261557832206\n",
            "  (0, 790)\t0.2945492827000402\n",
            "  (0, 939)\t0.2787178656358544\n",
            "  (0, 976)\t0.3929030288664009\n",
            "  (0, 1087)\t0.1590725296477077\n",
            "  (0, 214)\t0.384278316914726\n",
            "  (0, 290)\t0.42675196410684546\n",
            "  (0, 310)\t0.09456800490558967\n",
            "  (0, 453)\t0.42675196410684546\n",
            "  (0, 486)\t0.10147876478095012\n",
            "  (0, 599)\t0.2201392895696019\n",
            "  (0, 686)\t0.46046519451125834\n",
            "  (0, 773)\t0.29866849267548295\n",
            "  (0, 1088)\t0.34519889842270446\n",
            "  (0, 64)\t0.29758942365223656\n",
            "  (0, 268)\t0.45719752518264517\n",
            "  (0, 310)\t0.10131472480691577\n",
            "  (0, 394)\t0.3593342657866509\n",
            "  (0, 426)\t0.29758942365223656\n",
            "  (0, 486)\t0.10871851571566776\n",
            "  (0, 660)\t0.4116937009123579\n",
            "  (0, 681)\t0.39545268304823084\n",
            "  (0, 751)\t0.36982625817539655\n",
            "  (0, 59)\t0.2494217585375952\n",
            "  (0, 81)\t0.4524281995392851\n",
            "  (0, 310)\t0.09291740766394044\n",
            "  (0, 339)\t0.26431666429423034\n",
            "  (0, 664)\t0.3775710935725577\n",
            "  (0, 676)\t0.4524281995392851\n",
            "  (0, 777)\t0.36267618781592253\n",
            "  (0, 1089)\t0.41930340245508624\n",
            "  (0, 49)\t0.21686622378006956\n",
            "  (0, 60)\t0.26503875460332427\n",
            "  (0, 78)\t0.424393800481591\n",
            "  (0, 107)\t0.21686622378006956\n",
            "  (0, 197)\t0.3712754518555021\n",
            "  (0, 296)\t0.31815710322941315\n",
            "  (0, 308)\t0.3283900916406574\n",
            "  (0, 310)\t0.08715984505287873\n",
            "  (0, 488)\t0.23722800809413083\n",
            "  (0, 757)\t0.2752717430145685\n",
            "  (0, 758)\t0.3402032098090069\n",
            "  (0, 1097)\t0.2308381770498052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(queries_vector[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3w2xpV-05fw",
        "outputId": "60c6d43d-8d81-466d-f826-0da46fa6c755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 214)\t0.384278316914726\n",
            "  (0, 290)\t0.42675196410684546\n",
            "  (0, 310)\t0.09456800490558967\n",
            "  (0, 453)\t0.42675196410684546\n",
            "  (0, 486)\t0.10147876478095012\n",
            "  (0, 599)\t0.2201392895696019\n",
            "  (0, 686)\t0.46046519451125834\n",
            "  (0, 773)\t0.29866849267548295\n",
            "  (0, 1088)\t0.34519889842270446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save queries_vector as a csv file, the first column needs to be the index of the vector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming queries_vector is already defined as in your provided code\n",
        "\n",
        "# Create an empty list to store the data for the CSV file\n",
        "data_for_csv = []\n",
        "\n",
        "# Iterate through the queries_vector and their corresponding indices\n",
        "for i, query_vector in enumerate(queries_vector):\n",
        "    # Convert the sparse matrix to a dense array\n",
        "    dense_vector = query_vector.toarray()[0]\n",
        "\n",
        "    # Create a row for the CSV, with the index as the first element\n",
        "    row = [i] + list(dense_vector)\n",
        "    data_for_csv.append(row)\n",
        "\n",
        "# Create column names\n",
        "column_names = ['index'] + [f'feature_{i}' for i in range(len(dense_vector))]\n",
        "\n",
        "# Create a pandas DataFrame from the data\n",
        "df = pd.DataFrame(data_for_csv, columns=column_names)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('queries_vector.csv', index=False)"
      ],
      "metadata": {
        "id": "w5V-xyb20tg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargar el vectorizador entrenado (asegúrate de haberlo guardado previamente)\n",
        "import joblib\n",
        "tfidf_vectorizer = joblib.load('count_vectorizer.joblib')\n",
        "\n",
        "queries_vector_2 = []\n",
        "\n",
        "for query in queries_procesadas:\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "    print(query_vector)\n",
        "    queries_vector_2.append(query_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP-hIm8l1Ody",
        "outputId": "511ded7a-1605-4242-d2f2-e964cf89dc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 107)\t1\n",
            "  (0, 180)\t2\n",
            "  (0, 310)\t1\n",
            "  (0, 371)\t1\n",
            "  (0, 589)\t1\n",
            "  (0, 688)\t1\n",
            "  (0, 790)\t1\n",
            "  (0, 939)\t1\n",
            "  (0, 976)\t1\n",
            "  (0, 1087)\t1\n",
            "  (0, 214)\t1\n",
            "  (0, 290)\t1\n",
            "  (0, 310)\t1\n",
            "  (0, 453)\t1\n",
            "  (0, 486)\t1\n",
            "  (0, 599)\t1\n",
            "  (0, 686)\t1\n",
            "  (0, 773)\t1\n",
            "  (0, 1088)\t1\n",
            "  (0, 64)\t1\n",
            "  (0, 268)\t1\n",
            "  (0, 310)\t1\n",
            "  (0, 394)\t1\n",
            "  (0, 426)\t1\n",
            "  (0, 486)\t1\n",
            "  (0, 660)\t1\n",
            "  (0, 681)\t1\n",
            "  (0, 751)\t1\n",
            "  (0, 59)\t1\n",
            "  (0, 81)\t1\n",
            "  (0, 310)\t1\n",
            "  (0, 339)\t1\n",
            "  (0, 664)\t1\n",
            "  (0, 676)\t1\n",
            "  (0, 777)\t1\n",
            "  (0, 1089)\t1\n",
            "  (0, 49)\t1\n",
            "  (0, 60)\t1\n",
            "  (0, 78)\t1\n",
            "  (0, 107)\t1\n",
            "  (0, 197)\t1\n",
            "  (0, 296)\t1\n",
            "  (0, 308)\t1\n",
            "  (0, 310)\t1\n",
            "  (0, 488)\t1\n",
            "  (0, 757)\t1\n",
            "  (0, 758)\t1\n",
            "  (0, 1097)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save queries_vector as a csv file, the first column needs to be the index of the vector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming queries_vector is already defined as in your provided code\n",
        "\n",
        "# Create an empty list to store the data for the CSV file\n",
        "data_for_csv = []\n",
        "\n",
        "# Iterate through the queries_vector and their corresponding indices\n",
        "for i, query_vector in enumerate(queries_vector_2):\n",
        "    # Convert the sparse matrix to a dense array\n",
        "    dense_vector = query_vector.toarray()[0]\n",
        "\n",
        "    # Create a row for the CSV, with the index as the first element\n",
        "    row = [i] + list(dense_vector)\n",
        "    data_for_csv.append(row)\n",
        "\n",
        "# Create column names\n",
        "column_names = ['index'] + [f'feature_{i}' for i in range(len(dense_vector))]\n",
        "\n",
        "# Create a pandas DataFrame from the data\n",
        "df = pd.DataFrame(data_for_csv, columns=column_names)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('queries_vector_2.csv', index=False)"
      ],
      "metadata": {
        "id": "oxfyYD-A1b_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}