{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Document Generation #"
      ],
      "metadata": {
        "id": "vQnx559X3xkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMA9U7Xd3BUx"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_document(zip_path, extract_to='documentos'):\n",
        "    # Create the target directory if it doesn't exist\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"Files extracted to {extract_to}\")\n",
        "\n",
        "# Example usage\n",
        "zip_path = 'Documentos.zip'  # Replace with your zip file path\n",
        "unzip_document(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: do a pip install unicode\n",
        "\n",
        "!pip install unidecode\n",
        "!python -m spacy download es_core_news_lg"
      ],
      "metadata": {
        "id": "h-kX41jf3L4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: delete the fder documentos_procesados\n",
        "\n",
        "import shutil\n",
        "\n",
        "def delete_directory(dir_path):\n",
        "    try:\n",
        "        shutil.rmtree(dir_path)\n",
        "        print(f\"Directory '{dir_path}' deleted successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Directory '{dir_path}' not found.\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting directory '{dir_path}': {e}\")\n",
        "\n",
        "# Example usage\n",
        "delete_directory('documentos_procesados')"
      ],
      "metadata": {
        "id": "3KvUxDgN3NXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize spaCy with Spanish model\n",
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "# Initialize Spanish stopwords\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Define source and destination folders\n",
        "carpeta_origen = 'documentos'\n",
        "carpeta_destino = 'documentos_procesados'\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(carpeta_destino, exist_ok=True)\n",
        "\n",
        "def preprocesar_nombre_archivo(nombre_archivo):\n",
        "    # Split the filename by '_'\n",
        "    partes = nombre_archivo.split('_')\n",
        "\n",
        "    # Process each part\n",
        "    partes_procesadas = []\n",
        "    for parte in partes:\n",
        "        # Remove non-alphanumeric characters\n",
        "        parte = re.sub(r'\\W+', ' ', parte)\n",
        "\n",
        "        # Convert to lowercase and remove accents\n",
        "        parte = unidecode(parte.lower())\n",
        "\n",
        "        # Process with spaCy\n",
        "        doc = nlp(parte)\n",
        "\n",
        "        # Filter stopwords and get lemmas\n",
        "        palabras = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "\n",
        "        # Join processed words for this part\n",
        "        parte_procesado = '_'.join(palabras)\n",
        "\n",
        "        if parte_procesado:\n",
        "            partes_procesadas.append(parte_procesado)\n",
        "\n",
        "    # Join all processed parts\n",
        "    nombre_procesado = '_'.join(partes_procesadas)\n",
        "\n",
        "    return nombre_procesado\n",
        "\n",
        "def preprocesar_texto(texto):\n",
        "    # Convert to lowercase and remove accents\n",
        "    texto = unidecode(texto.lower())\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    texto = re.sub(r'\\W+', ' ', texto)\n",
        "\n",
        "    # Process with spaCy\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Filter words and get lemmas\n",
        "    palabras = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "\n",
        "    # Join processed words\n",
        "    texto_procesado = ' '.join(palabras)\n",
        "\n",
        "    return texto_procesado\n",
        "\n",
        "def procesar_archivos(carpeta_origen, carpeta_destino):\n",
        "    for archivo in os.listdir(carpeta_origen):\n",
        "        if archivo.endswith('.txt'):\n",
        "            ruta_origen = os.path.join(carpeta_origen, archivo)\n",
        "\n",
        "            # Preprocess filename\n",
        "            nombre_procesado = preprocesar_nombre_archivo(archivo)\n",
        "            ruta_destino = os.path.join(carpeta_destino, f\"{archivo}.txt\")\n",
        "\n",
        "            try:\n",
        "                # Read original file\n",
        "                with open(ruta_origen, 'r', encoding='utf-8') as f:\n",
        "                    contenido = f.read()\n",
        "\n",
        "                # Process file content\n",
        "                contenido_procesado = preprocesar_texto(contenido)\n",
        "\n",
        "                # Save processed content with processed filename\n",
        "                with open(ruta_destino, 'w', encoding='utf-8') as f:\n",
        "                    f.write(contenido_procesado)\n",
        "\n",
        "                print(f\"Processed: {archivo} -> {nombre_procesado}.txt\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {archivo}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    procesar_archivos(carpeta_origen, carpeta_destino)"
      ],
      "metadata": {
        "id": "93vAAW5k3Rb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib  # Para guardar el vectorizador\n",
        "\n",
        "def calculate_tf_idf(folder_path, output_file, vectorizer_file):\n",
        "    \"\"\"\n",
        "    Calculates TF-IDF vectors for documents in a folder, saves them to a CSV file,\n",
        "    and saves the TF-IDF vectorizer as a .joblib file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Store filenames and text contents\n",
        "    filenames = []\n",
        "    documents = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "                    filenames.append(filename)\n",
        "                    documents.append(text)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "    # Fit and transform the documents\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Save the TF-IDF vectorizer to a .joblib file\n",
        "    joblib.dump(vectorizer, vectorizer_file)\n",
        "    print(f\"TF-IDF vectorizer saved to {vectorizer_file}\")\n",
        "\n",
        "    # Create a DataFrame from the TF-IDF matrix\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Insert filenames as the first column\n",
        "    tfidf_df.insert(0, 'filename', filenames)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    tfidf_df.to_csv(output_file, index=False)\n",
        "    print(f\"TF-IDF vectors saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "folder_path = 'documentos_procesados'\n",
        "output_file = 'tf_idf_documentos.csv'\n",
        "vectorizer_file = 'tfidf_vectorizer.joblib'\n",
        "calculate_tf_idf(folder_path, output_file, vectorizer_file)\n"
      ],
      "metadata": {
        "id": "7Re0brXT3TrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import joblib  # Para guardar el vectorizador\n",
        "\n",
        "def calculate_log_tf_idf(folder_path, output_file, vectorizer_file):\n",
        "    \"\"\"\n",
        "    Calculates logarithmic TF and standard IDF vectors for documents in a folder,\n",
        "    and saves the CountVectorizer as a .joblib file.\n",
        "    \"\"\"\n",
        "    # Store filenames and text contents\n",
        "    filenames = []\n",
        "    documents = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "                    filenames.append(filename)\n",
        "                    documents.append(text)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "    # Create count vectorizer\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    term_freq_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Save the CountVectorizer to a .joblib file\n",
        "    joblib.dump(count_vectorizer, vectorizer_file)\n",
        "    print(f\"CountVectorizer saved to {vectorizer_file}\")\n",
        "\n",
        "    # Get feature names\n",
        "    feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Calculate logarithmic TF\n",
        "    log_tf_matrix = np.log1p(term_freq_matrix.toarray())\n",
        "\n",
        "    # Calculate IDF\n",
        "    doc_count = len(documents)\n",
        "    idf_vector = np.log(doc_count / (np.sum(term_freq_matrix.toarray() > 0, axis=0) + 1))\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tfidf_matrix = log_tf_matrix * idf_vector\n",
        "\n",
        "    # Create DataFrame\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix, columns=feature_names)\n",
        "    tfidf_df.insert(0, 'filename', filenames)\n",
        "\n",
        "    # Save to CSV\n",
        "    tfidf_df.to_csv(output_file, index=False)\n",
        "    print(f\"Logarithmic TF-IDF vectors saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "folder_path = 'documentos_procesados'\n",
        "output_file = 'tf_idf_documentos_2.csv'\n",
        "vectorizer_file = 'count_vectorizer.joblib'\n",
        "calculate_log_tf_idf(folder_path, output_file, vectorizer_file)\n"
      ],
      "metadata": {
        "id": "Rgg6u0ZP3Vad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"curso de programación, diseño gráfico y cocina internacional con técnicas avanzadas de cocina y recetas modernas\",\n",
        "    \"Curso de escritura creativa y técnica para estudiantes de música interesados en mejorar sus habilidades de composición\",\n",
        "    \"curso de idiomas inicial para estudiantes que quieren aprender inglés de manera interactiva y efectiva con enfoque en conversación\",\n",
        "    \"curso sobre inteligencia artificial, tecnología de vanguardia, innovación disruptiva y desarrollo de aplicaciones inteligentes en el mercado\",\n",
        "    \"curso para estudios de matemáticas aplicadas y artes liberales, combinando teoría matemática avanzada con análisis crítico cultural\",\n",
        "]\n",
        "\n",
        "queries_procesadas = []\n",
        "\n",
        "for query in queries:\n",
        "    # Convert to lowercase and remove accents\n",
        "    query_pre = unidecode(query.lower())\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    query_pre = re.sub(r'\\W+', ' ', query_pre)\n",
        "\n",
        "    # Process with spaCy\n",
        "    doc = nlp(query_pre)\n",
        "\n",
        "    # Filter words and get lemmas\n",
        "    palabras = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "\n",
        "    # Join processed words\n",
        "    query_preprocesada = ' '.join(palabras)\n",
        "\n",
        "    queries_procesadas.append(query_preprocesada)\n",
        "\n",
        "for query in queries_procesadas:\n",
        "    print(query)\n"
      ],
      "metadata": {
        "id": "eacBqeOH3XR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargar el vectorizador entrenado (asegúrate de haberlo guardado previamente)\n",
        "import joblib\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
        "\n",
        "queries_vector = []\n",
        "\n",
        "for query in queries_procesadas:\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "    print(query_vector)\n",
        "    queries_vector.append(query_vector)"
      ],
      "metadata": {
        "id": "gsY_KyG13ZMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save queries_vector as a csv file, the first column needs to be the index of the vector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming queries_vector is already defined as in your provided code\n",
        "\n",
        "# Create an empty list to store the data for the CSV file\n",
        "data_for_csv = []\n",
        "\n",
        "# Iterate through the queries_vector and their corresponding indices\n",
        "for i, query_vector in enumerate(queries_vector):\n",
        "    # Convert the sparse matrix to a dense array\n",
        "    dense_vector = query_vector.toarray()[0]\n",
        "\n",
        "    # Create a row for the CSV, with the index as the first element\n",
        "    row = [i] + list(dense_vector)\n",
        "    data_for_csv.append(row)\n",
        "\n",
        "# Create column names\n",
        "column_names = ['index'] + [f'feature_{i}' for i in range(len(dense_vector))]\n",
        "\n",
        "# Create a pandas DataFrame from the data\n",
        "df = pd.DataFrame(data_for_csv, columns=column_names)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('queries_vector.csv', index=False)"
      ],
      "metadata": {
        "id": "rCkidaJr3bjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargar el vectorizador entrenado (asegúrate de haberlo guardado previamente)\n",
        "import joblib\n",
        "tfidf_vectorizer = joblib.load('count_vectorizer.joblib')\n",
        "\n",
        "queries_vector_2 = []\n",
        "\n",
        "for query in queries_procesadas:\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "    print(query_vector)\n",
        "    queries_vector_2.append(query_vector)"
      ],
      "metadata": {
        "id": "_LcZ3Ezn3dWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save queries_vector as a csv file, the first column needs to be the index of the vector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming queries_vector is already defined as in your provided code\n",
        "\n",
        "# Create an empty list to store the data for the CSV file\n",
        "data_for_csv = []\n",
        "\n",
        "# Iterate through the queries_vector and their corresponding indices\n",
        "for i, query_vector in enumerate(queries_vector_2):\n",
        "    # Convert the sparse matrix to a dense array\n",
        "    dense_vector = query_vector.toarray()[0]\n",
        "\n",
        "    # Create a row for the CSV, with the index as the first element\n",
        "    row = [i] + list(dense_vector)\n",
        "    data_for_csv.append(row)\n",
        "\n",
        "# Create column names\n",
        "column_names = ['index'] + [f'feature_{i}' for i in range(len(dense_vector))]\n",
        "\n",
        "# Create a pandas DataFrame from the data\n",
        "df = pd.DataFrame(data_for_csv, columns=column_names)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('queries_vector_2.csv', index=False)"
      ],
      "metadata": {
        "id": "Wilb0J_P3hCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction #"
      ],
      "metadata": {
        "id": "zBnrK5WC32zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For tf_idf ##"
      ],
      "metadata": {
        "id": "cUUKA0rw3450"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save my csv file in a dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your CSV file is named 'your_file.csv'\n",
        "# Replace 'your_file.csv' with the actual filename\n",
        "try:\n",
        "  df = pd.read_csv('tf_idf_documentos.csv')\n",
        "  print(\"File loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: File not found. Please check the filename and ensure it exists in the current directory.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "  print(\"Error: The CSV file is empty.\")\n",
        "except pd.errors.ParserError:\n",
        "  print(\"Error: There was an issue parsing the CSV file. Please check its format.\")\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")\n",
        "else:\n",
        "  # You can now work with the DataFrame 'df'\n",
        "  print(df.head()) # Print the first few rows to verify\n",
        "  # ... your further code ...\n"
      ],
      "metadata": {
        "id": "MTg7TAfo38im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: visualize my df with tsne\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your data for t-SNE is in columns 1 onwards (exclude the first column if it's an index)\n",
        "X = df.iloc[:, 1:]  # Adjust the slicing if your data is in different columns\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)  # Adjust parameters as needed\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
        "plt.title('t-SNE Visualization')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "92V9fILx4AO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def read_and_prepare_data(file_path, normalization_method='minmax'):\n",
        "   \"\"\"\n",
        "   Read CSV, remove first column, and normalize data\n",
        "\n",
        "   Args:\n",
        "       file_path (str): Path to CSV file\n",
        "       normalization_method (str): 'minmax' or 'standard'\n",
        "\n",
        "   Returns:\n",
        "       tuple: Normalized DataFrame, first column\n",
        "   \"\"\"\n",
        "   try:\n",
        "       # Read CSV file\n",
        "       df = pd.read_csv(file_path)\n",
        "\n",
        "       # Extract first column\n",
        "       first_column = df.iloc[:, 0]\n",
        "\n",
        "       # Remove first column from DataFrame\n",
        "       X = df.iloc[:, 1:]\n",
        "\n",
        "       # Normalize data\n",
        "       if normalization_method == 'minmax':\n",
        "           scaler = MinMaxScaler()\n",
        "           X_normalized = pd.DataFrame(\n",
        "               scaler.fit_transform(X),\n",
        "               columns=X.columns\n",
        "           )\n",
        "       elif normalization_method == 'standard':\n",
        "           from sklearn.preprocessing import StandardScaler\n",
        "           scaler = StandardScaler()\n",
        "           X_normalized = pd.DataFrame(\n",
        "               scaler.fit_transform(X),\n",
        "               columns=X.columns\n",
        "           )\n",
        "       else:\n",
        "           raise ValueError(\"Invalid normalization method\")\n",
        "\n",
        "       return X_normalized, first_column\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Error processing data: {e}\")\n",
        "       return None, None"
      ],
      "metadata": {
        "id": "E3peirmU4B_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'tf_idf_documentos.csv'\n",
        "X, first_column = read_and_prepare_data(file_path, 'standard')\n"
      ],
      "metadata": {
        "id": "dM0dQkp34Du0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE, Isomap\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "import math\n",
        "\n",
        "def run_isomap_with_tsne(X_scaled, param_grid, first_column):\n",
        "    \"\"\"Run Isomap with multiple parameter combinations, plot reduced space with t-SNE, and save results.\"\"\"\n",
        "    results = []\n",
        "    min_error = float('inf')  # Track the minimum reconstruction error\n",
        "    best_idx = None           # Track the index of the best parameter combination\n",
        "\n",
        "    # Folder for saving CSVs\n",
        "    output_folder = \"Isomap_2\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get the total number of parameter combinations\n",
        "    param_combinations = list(product(param_grid['n_components'], param_grid['n_neighbors']))\n",
        "    num_combinations = len(param_combinations)\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    cols = 3  # Number of columns\n",
        "    rows = math.ceil(num_combinations / cols)  # Number of rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "    axes = axes.flatten()  # Flatten axes for easy iteration\n",
        "\n",
        "    for idx, (n_components, n_neighbors) in enumerate(param_combinations):\n",
        "        try:\n",
        "            print(f\"Running Isomap with n_components={n_components}, n_neighbors={n_neighbors}\")\n",
        "\n",
        "            # Validate parameters\n",
        "            if n_components > X_scaled.shape[1]:\n",
        "                print(f\"Skipping n_components={n_components}: exceeds feature dimensions.\")\n",
        "                continue\n",
        "            if n_neighbors >= X_scaled.shape[0]:\n",
        "                print(f\"Skipping n_neighbors={n_neighbors}: exceeds number of data points.\")\n",
        "                continue\n",
        "\n",
        "            # Run Isomap\n",
        "            isomap = Isomap(n_components=n_components, n_neighbors=n_neighbors, n_jobs=-1)\n",
        "            transformed_data = isomap.fit_transform(X_scaled)\n",
        "\n",
        "            # Calculate reconstruction error\n",
        "            original_distances = pairwise_distances(X_scaled)\n",
        "            reduced_distances = pairwise_distances(transformed_data)\n",
        "            reconstruction_error = np.mean((original_distances - reduced_distances) ** 2)\n",
        "\n",
        "            # Update the best error and index\n",
        "            if reconstruction_error < min_error:\n",
        "                min_error = reconstruction_error\n",
        "                best_idx = idx\n",
        "\n",
        "            # Create DataFrame with first column\n",
        "            output_df = pd.DataFrame(transformed_data)\n",
        "            output_df.insert(0, 'original_column', first_column)\n",
        "\n",
        "            # Save the reduced space to a CSV file\n",
        "            file_name = f\"isomap_ncomp{n_components}_nneigh{n_neighbors}_error{reconstruction_error:.4f}.csv\"\n",
        "            file_path = os.path.join(output_folder, file_name)\n",
        "            output_df.to_csv(file_path, index=False)\n",
        "            print(f\"Saved Isomap result to {file_path}\")\n",
        "\n",
        "            # Apply t-SNE on the Isomap-transformed data\n",
        "            tsne = TSNE(n_components=2, random_state=42)\n",
        "            tsne_result = tsne.fit_transform(transformed_data)\n",
        "\n",
        "            # Plot the reduced space in the corresponding subplot\n",
        "            ax = axes[idx]\n",
        "            ax.scatter(tsne_result[:, 0], tsne_result[:, 1], c='blue', alpha=0.6, edgecolors='k')\n",
        "            ax.set_title(f\"n_components={n_components}, n_neighbors={n_neighbors}\\nMSE: {reconstruction_error:.4f}\")\n",
        "            ax.set_xlabel(\"t-SNE Dimension 1\")\n",
        "            ax.set_ylabel(\"t-SNE Dimension 2\")\n",
        "            ax.grid(True)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'params': {'n_components': n_components, 'n_neighbors': n_neighbors},\n",
        "                'transformed_data': transformed_data,\n",
        "                'reconstruction_error': reconstruction_error\n",
        "            })\n",
        "\n",
        "        except ValueError as ve:\n",
        "            print(f\"ValueError for n_components={n_components}, n_neighbors={n_neighbors}: {str(ve)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error for n_components={n_components}, n_neighbors={n_neighbors}: {str(e)}\")\n",
        "\n",
        "    # Highlight the best subplot\n",
        "    if best_idx is not None:\n",
        "        axes[best_idx].set_title(\n",
        "            axes[best_idx].get_title(),\n",
        "            fontweight='bold',\n",
        "            color='red'\n",
        "        )\n",
        "\n",
        "    # Remove empty subplots\n",
        "    for i in range(len(param_combinations), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    # Adjust layout and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "wwf4VWHV4E1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "h5uis3Ie4Gua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_components': [2, 3, 10, 20, 30, 40, 50, 60], 'n_neighbors': [10, 20, 30, 40, 50, 60]}\n",
        "results = run_isomap_with_tsne(X, param_grid, first_column)\n",
        "\n",
        "# Display reconstruction errors\n",
        "for result in results:\n",
        "    print(f\"Params: {result['params']}, Reconstruction Error: {result['reconstruction_error']}\")\n"
      ],
      "metadata": {
        "id": "LTkJv52a4IHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras import regularizers\n",
        "import math\n",
        "\n",
        "def build_autoencoder(X_scaled, params):\n",
        "    \"\"\"Build autoencoder with given parameters.\"\"\"\n",
        "    input_dim = X_scaled.shape[1]\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder\n",
        "    if params['type'] == 'sparse':\n",
        "        encoded = Dense(params['encoding_dim'], activation='relu',\n",
        "                        activity_regularizer=regularizers.l1(params['parameter']))(input_layer)\n",
        "    else:  # denoising\n",
        "        encoded = Dense(params['encoding_dim'], activation='relu')(\n",
        "            Dropout(params['parameter'])(input_layer))\n",
        "\n",
        "    # Decoder\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "def run_autoencoder(X_scaled, params):\n",
        "    \"\"\"Run autoencoder with given parameters.\"\"\"\n",
        "    try:\n",
        "        # Split data without labels\n",
        "        X_train, X_test = train_test_split(\n",
        "            X_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Build and train autoencoder\n",
        "        autoencoder = build_autoencoder(X_scaled, params)\n",
        "        history = autoencoder.fit(\n",
        "            X_train, X_train,\n",
        "            epochs=150,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Get latent space\n",
        "        encoder = Model(autoencoder.input, autoencoder.layers[1].output)\n",
        "        latent_space = encoder.predict(X_scaled)\n",
        "\n",
        "        # Calculate reconstruction error\n",
        "        reconstructed = autoencoder.predict(X_scaled)\n",
        "        reconstruction_error = np.mean((X_scaled - reconstructed) ** 2)\n",
        "\n",
        "        return {\n",
        "            'params': params,\n",
        "            'history': history.history,\n",
        "            'latent_space': latent_space,\n",
        "            'reconstruction_error': reconstruction_error\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Autoencoder with params {params}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def run_autoencoders_with_tsne(X_scaled, param_grid, first_column):\n",
        "    \"\"\"Run autoencoders with multiple parameter combinations and plot results with t-SNE.\"\"\"\n",
        "    results = []\n",
        "    min_error = float('inf')  # Track the minimum reconstruction error\n",
        "    best_idx = None           # Track the index of the best parameter combination\n",
        "\n",
        "    if param_grid['type'][0] == 'sparse':\n",
        "        # Get the total number of parameter combinations\n",
        "        param_combinations = list(product(\n",
        "            param_grid['type'],\n",
        "            param_grid['encoding_dim'],\n",
        "            param_grid['sparsity']\n",
        "        ))\n",
        "    else:\n",
        "        param_combinations = list(product(\n",
        "            param_grid['type'],\n",
        "            param_grid['encoding_dim'],\n",
        "            param_grid['dropout_rate']\n",
        "        ))\n",
        "    num_combinations = len(param_combinations)\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    cols = 3  # Number of columns\n",
        "    rows = math.ceil(num_combinations / cols)  # Number of rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "    axes = axes.flatten()  # Flatten axes for easy iteration\n",
        "\n",
        "    for idx, (ae_type, encoding_dim, parameter) in enumerate(param_combinations):\n",
        "        try:\n",
        "            print(f\"Running Autoencoder with type={ae_type}, encoding_dim={encoding_dim}, parameter={parameter}\")\n",
        "\n",
        "            params = {\n",
        "                'type': ae_type,\n",
        "                'encoding_dim': encoding_dim,\n",
        "                'parameter': parameter\n",
        "            }\n",
        "\n",
        "            # Build and train the autoencoder\n",
        "            result = run_autoencoder(X_scaled, params)\n",
        "            if result is None:\n",
        "                continue\n",
        "\n",
        "            latent_space = result['latent_space']\n",
        "            reconstruction_error = result['reconstruction_error']\n",
        "\n",
        "            # Update the best error and index\n",
        "            if reconstruction_error < min_error:\n",
        "                min_error = reconstruction_error\n",
        "                best_idx = idx\n",
        "\n",
        "            # Save latent space to CSV\n",
        "            output_folder = \"Autoencoders_2\"\n",
        "            os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "            # Create DataFrame with first column\n",
        "            output_df = pd.DataFrame(latent_space)\n",
        "            output_df.insert(0, 'original_column', first_column)\n",
        "\n",
        "            file_name = f\"autoencoder_type{ae_type}_dim{encoding_dim}_param{parameter}_error{reconstruction_error:.4f}.csv\"\n",
        "            file_path = os.path.join(output_folder, file_name)\n",
        "            pd.DataFrame(output_df).to_csv(file_path, index=False)\n",
        "            print(f\"Saved latent space to {file_path}\")\n",
        "\n",
        "            # Reduce latent space to 2D with t-SNE\n",
        "            tsne = TSNE(n_components=2, random_state=42)\n",
        "            tsne_result = tsne.fit_transform(latent_space)\n",
        "\n",
        "            # Plot the reduced latent space\n",
        "            ax = axes[idx]\n",
        "            ax.scatter(tsne_result[:, 0], tsne_result[:, 1], c='blue', alpha=0.6, edgecolors='k')\n",
        "            if ae_type == 'sparse':\n",
        "                ax.set_title(f\"type={ae_type}, dim={encoding_dim}, sparsity={parameter}\\nMSE: {reconstruction_error:.4f}\")\n",
        "            else:\n",
        "                ax.set_title(f\"type={ae_type}, dim={encoding_dim}, dropout_rate = {parameter}\\nMSE: {reconstruction_error:.4f}\")\n",
        "            ax.set_xlabel(\"t-SNE Dimension 1\")\n",
        "            ax.set_ylabel(\"t-SNE Dimension 2\")\n",
        "            ax.grid(True)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'params': params,\n",
        "                'reconstruction_error': reconstruction_error\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error for type={ae_type}, dim={encoding_dim}: {str(e)}\")\n",
        "\n",
        "    # Highlight the best subplot\n",
        "    if best_idx is not None:\n",
        "        axes[best_idx].set_title(\n",
        "            axes[best_idx].get_title(),\n",
        "            fontweight='bold',\n",
        "            color='red'\n",
        "        )\n",
        "\n",
        "    # Remove empty subplots\n",
        "    for i in range(len(param_combinations), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    # Adjust layout and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "6ccI2vPM4Jaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_sparse = {\n",
        "    'type': ['sparse'],\n",
        "    'encoding_dim': [2, 3, 10, 20, 30, 40, 50, 60],\n",
        "    'sparsity': [1e-4, 1e-5, 1e-6, 1e-7]  # Only used for sparse autoencoder\n",
        "}\n",
        "\n",
        "param_grid_denoising = {\n",
        "    'type': ['denoising'],\n",
        "    'encoding_dim': [2, 3, 10, 20, 30, 40, 50, 60],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3, 0.4]  # Only used for denoising autoencoder\n",
        "}\n",
        "\n",
        "# Run the function\n",
        "results_sparse = run_autoencoders_with_tsne(X, param_grid_sparse, first_column)\n",
        "results_denoising = run_autoencoders_with_tsne(X, param_grid_denoising, first_column)\n"
      ],
      "metadata": {
        "id": "ZuiM0WAO4LPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from itertools import product\n",
        "import math\n",
        "\n",
        "def run_umap_with_tsne(X_scaled, param_grid, first_column):\n",
        "    \"\"\"Run UMAP with different parameter combinations and plot reduced space with t-SNE.\"\"\"\n",
        "    results = []\n",
        "    min_error = float('inf')  # Track the minimum reconstruction error\n",
        "    best_idx = None           # Track the index of the best parameter combination\n",
        "\n",
        "    # Folder for saving CSVs\n",
        "    output_folder = \"Umap_2\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get the total number of parameter combinations\n",
        "    param_combinations = list(product(\n",
        "        param_grid['n_components'],\n",
        "        param_grid['n_neighbors'],\n",
        "        param_grid['min_dist'],\n",
        "        param_grid['metric']\n",
        "    ))\n",
        "    num_combinations = len(param_combinations)\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    cols = 3  # Number of columns\n",
        "    rows = math.ceil(num_combinations / cols)  # Number of rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "    axes = axes.flatten()  # Flatten axes for easy iteration\n",
        "\n",
        "    # Iterate through each combination of n_components, n_neighbors, and min_dist\n",
        "    for idx, (n_components, n_neighbors, min_dist, metric) in enumerate(param_combinations):\n",
        "        try:\n",
        "            print(f\"Running UMAP with n_components={n_components}, n_neighbors={n_neighbors}, min_dist={min_dist}, metric={metric}\")\n",
        "\n",
        "            # Validate parameters\n",
        "            if n_components > X_scaled.shape[1]:\n",
        "                print(f\"Skipping n_components={n_components}: exceeds feature dimensions.\")\n",
        "                continue\n",
        "            if n_neighbors >= X_scaled.shape[0]:\n",
        "                print(f\"Skipping n_neighbors={n_neighbors}: exceeds number of data points.\")\n",
        "                continue\n",
        "\n",
        "            # Run UMAP with the new parameter\n",
        "            umap_model = umap.UMAP(\n",
        "                n_neighbors=n_neighbors,\n",
        "                n_components=n_components,\n",
        "                min_dist=min_dist,\n",
        "                metric=metric,\n",
        "                random_state=42\n",
        "            )\n",
        "            umap_transformed_data = umap_model.fit_transform(X_scaled)\n",
        "\n",
        "            # Apply t-SNE on the UMAP-transformed data\n",
        "            tsne = TSNE(n_components=2, random_state=42)\n",
        "            tsne_result = tsne.fit_transform(umap_transformed_data)\n",
        "\n",
        "            # Calculate reconstruction error (using pairwise distance as a proxy)\n",
        "            original_distances = pairwise_distances(X_scaled)\n",
        "            reduced_distances = pairwise_distances(umap_transformed_data)\n",
        "            reconstruction_error = np.mean((original_distances - reduced_distances) ** 2)\n",
        "\n",
        "            # Update the best error and index\n",
        "            if reconstruction_error < min_error:\n",
        "                min_error = reconstruction_error\n",
        "                best_idx = idx\n",
        "\n",
        "            # Create DataFrame with first column\n",
        "            output_df = pd.DataFrame(umap_transformed_data)\n",
        "            output_df.insert(0, 'original_column', first_column)\n",
        "\n",
        "            # Save the reduced space to a CSV file\n",
        "            file_name = f\"umap_ncomp{n_components}_nneigh{n_neighbors}_mindist{min_dist}_metric{metric}_error{reconstruction_error:.4f}.csv\"\n",
        "            file_path = os.path.join(output_folder, file_name)\n",
        "            pd.DataFrame(output_df).to_csv(file_path, index=False)\n",
        "            print(f\"Saved Umap result to {file_path}\")\n",
        "\n",
        "            # Plot the reduced space in the corresponding subplot\n",
        "            ax = axes[idx]\n",
        "            ax.scatter(tsne_result[:, 0], tsne_result[:, 1], c='blue', alpha=0.6, edgecolors='k')\n",
        "            ax.set_title(f\"n_components={n_components}, n_neighbors={n_neighbors}, min_dist={min_dist}, metric={metric}\\nError: {reconstruction_error:.4f}\")\n",
        "            ax.set_xlabel(\"t-SNE Dimension 1\")\n",
        "            ax.set_ylabel(\"t-SNE Dimension 2\")\n",
        "            ax.grid(True)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'params': {'n_components': n_components, 'n_neighbors': n_neighbors, 'min_dist': min_dist, 'metric': metric},\n",
        "                'transformed_data': umap_transformed_data,\n",
        "                'reconstruction_error': reconstruction_error\n",
        "            })\n",
        "\n",
        "\n",
        "        except ValueError as ve:\n",
        "            print(f\"ValueError for n_components={n_components}, n_neighbors={n_neighbors}, min_dist={min_dist}: {str(ve)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error for n_components={n_components}, n_neighbors={n_neighbors}, min_dist={min_dist}: {str(e)}\")\n",
        "\n",
        "    # Highlight the best subplot\n",
        "    if best_idx is not None:\n",
        "        axes[best_idx].set_title(\n",
        "            axes[best_idx].get_title(),\n",
        "            fontweight='bold',\n",
        "            color='red'\n",
        "        )\n",
        "\n",
        "    # Remove empty subplots\n",
        "    for i in range(len(param_combinations), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "3a1GEY9i4MvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to call the function with a parameter grid\n",
        "param_grid = {\n",
        "    'n_components': [2, 3, 5, 10, 20],\n",
        "    'n_neighbors': [20, 40, 60],\n",
        "    'min_dist': [0.1, 0.3, 0.7],\n",
        "    'metric': ['euclidean', 'cosine']\n",
        "}\n",
        "\n",
        "# Run the function\n",
        "results = run_umap_with_tsne(X, param_grid, first_column)\n"
      ],
      "metadata": {
        "id": "urSMD6ln4QQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf_2 ##"
      ],
      "metadata": {
        "id": "E625b6At4Tea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save my csv file in a dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your CSV file is named 'your_file.csv'\n",
        "# Replace 'your_file.csv' with the actual filename\n",
        "try:\n",
        "  df = pd.read_csv('tf_idf_documentos_2.csv')\n",
        "  print(\"File loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: File not found. Please check the filename and ensure it exists in the current directory.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "  print(\"Error: The CSV file is empty.\")\n",
        "except pd.errors.ParserError:\n",
        "  print(\"Error: There was an issue parsing the CSV file. Please check its format.\")\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")\n",
        "else:\n",
        "  # You can now work with the DataFrame 'df'\n",
        "  print(df.head()) # Print the first few rows to verify"
      ],
      "metadata": {
        "id": "OK8bHwKV4Rx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: visualize my df with tsne\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your data for t-SNE is in columns 1 onwards (exclude the first column if it's an index)\n",
        "X = df.iloc[:, 1:]  # Adjust the slicing if your data is in different columns\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)  # Adjust parameters as needed\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
        "plt.title('t-SNE Visualization')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mnq1Owj_4gAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'tf_idf_documentos_2.csv'\n",
        "X, first_column = read_and_prepare_data(file_path, 'standard')\n",
        "\n",
        "# Optional: print DataFrame details\n",
        "print(\"DataFrame shape:\", X.shape)\n",
        "print(\"First column details:\", first_column)"
      ],
      "metadata": {
        "id": "OHT2T8lq4hzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_components': [2, 3, 10, 20, 30, 40, 50, 60], 'n_neighbors': [10, 20, 30, 40, 50, 60]}\n",
        "results = run_isomap_with_tsne(X, param_grid, first_column)\n",
        "\n",
        "# Display reconstruction errors\n",
        "for result in results:\n",
        "    print(f\"Params: {result['params']}, Reconstruction Error: {result['reconstruction_error']}\")"
      ],
      "metadata": {
        "id": "O3C5jtVk4jSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_sparse = {\n",
        "    'type': ['sparse'],\n",
        "    'encoding_dim': [2, 3, 10, 20, 30, 40, 50, 60],\n",
        "    'sparsity': [1e-4, 1e-5, 1e-6, 1e-7]  # Only used for sparse autoencoder\n",
        "}\n",
        "\n",
        "param_grid_denoising = {\n",
        "    'type': ['denoising'],\n",
        "    'encoding_dim': [2, 3, 10, 20, 30, 40, 50, 60],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3, 0.4]  # Only used for denoising autoencoder\n",
        "}\n",
        "\n",
        "# Run the function\n",
        "results_sparse = run_autoencoders_with_tsne(X, param_grid_sparse, first_column)\n",
        "results_denoising = run_autoencoders_with_tsne(X, param_grid_denoising, first_column)\n"
      ],
      "metadata": {
        "id": "_3ICVlNe4k4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to call the function with a parameter grid\n",
        "param_grid = {\n",
        "    'n_components': [2, 3, 5, 10, 20],\n",
        "    'n_neighbors': [20, 40, 60],\n",
        "    'min_dist': [0.1, 0.3, 0.7],\n",
        "    'metric': ['euclidean', 'cosine']\n",
        "}\n",
        "\n",
        "# Run the function\n",
        "results = run_umap_with_tsne(X, param_grid, first_column)\n"
      ],
      "metadata": {
        "id": "VvFw-rjJ4msz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: zip my three folders Autoencoders, Isomap and Umap into a single zip file but keep each folder\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_folders(folder_names, zip_file_name):\n",
        "  \"\"\"Zips multiple folders into a single zip file, preserving folder structure.\"\"\"\n",
        "  import zipfile\n",
        "\n",
        "  try:\n",
        "    with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "      for folder_name in folder_names:\n",
        "        for root, _, files in os.walk(folder_name):\n",
        "          for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, os.path.dirname(folder_name)) # Preserve folder structure\n",
        "            zipf.write(file_path, arcname=arcname)\n",
        "    print(f\"Successfully zipped folders into '{zip_file_name}'\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"One or more folders not found.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "folder_names = [\"Autoencoders\", \"Isomap\", \"Umap\", \"Autoencoders_2\", \"Isomap_2\", \"Umap_2\"]  # Replace with your actual folder names\n",
        "zip_file_name = \"dimensionality_reduction_results.zip\"\n",
        "zip_folders(folder_names, zip_file_name)\n"
      ],
      "metadata": {
        "id": "Q9qDIB0P4od4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering #"
      ],
      "metadata": {
        "id": "s3fdnoY547N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (silhouette_score, calinski_harabasz_score, davies_bouldin_score)\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "\n",
        "def kmeans_clustering(X, k_range=range(2, 9), title_prefix=\"\", model_name=\"\"):\n",
        "    # Convertir X a DataFrame si no lo es\n",
        "    if isinstance(X, np.ndarray):\n",
        "        X = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(X.shape[1])])\n",
        "\n",
        "    # Inicializar listas para almacenar las métricas\n",
        "    inertia = []\n",
        "    silhouette_scores = []\n",
        "    calinski_scores = []\n",
        "    davies_bouldin_scores = []\n",
        "\n",
        "    # Configurar subplots para una cuadrícula de 3x3 con un tamaño de 15x15 pulgadas\n",
        "    fig, axes = plt.subplots(7, 3, figsize=(20, 50))\n",
        "    axes = axes.flatten()  # Aplanar para iterar fácilmente\n",
        "\n",
        "    for i, k in enumerate(k_range):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(X)\n",
        "\n",
        "        # Guardar la inercia (para el Elbow Plot)\n",
        "        inertia.append(kmeans.inertia_)\n",
        "\n",
        "        # Calcular y guardar las métricas\n",
        "        labels = kmeans.labels_\n",
        "        silhouette = silhouette_score(X, labels)\n",
        "        silhouette_scores.append(silhouette)\n",
        "\n",
        "        calinski = calinski_harabasz_score(X, labels)\n",
        "        calinski_scores.append(calinski)\n",
        "\n",
        "        davies_bouldin = davies_bouldin_score(X, labels)\n",
        "        davies_bouldin_scores.append(davies_bouldin)\n",
        "\n",
        "        # Guardar el modelo con nombre basado en las métricas\n",
        "        model_filename = f\"classification_model/{model_name}_k{k}_silh{silhouette:.2f}_calinski{calinski:.2f}_davies{davies_bouldin:.2f}.joblib\"\n",
        "        joblib.dump(kmeans, model_filename)\n",
        "        print(f\"Model saved as: {model_filename}\")\n",
        "\n",
        "        # Visualizar clustering en 2D con métricas\n",
        "        axes[i].scatter(X.iloc[:, 0], X.iloc[:, 1], c=labels, cmap='viridis', s=30, alpha=0.6)\n",
        "        axes[i].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "                        c='red', marker='x', s=100, linewidths=2, label='Centroids')\n",
        "        axes[i].set_title(f'{title_prefix}Clusters: {k}\\nSilhouette: {silhouette:.2f}\\n'\n",
        "                          f'Calinski: {calinski:.2f}\\nDavies-Bouldin: {davies_bouldin:.2f}')\n",
        "        axes[i].legend()\n",
        "\n",
        "    # Ocultar cualquier subplot adicional no utilizado\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Crear el Elbow Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_range, inertia, marker='o', color='b', label='Inercia (Elbow)')\n",
        "    plt.xlabel('Número de clusters (k)')\n",
        "    plt.ylabel('Inercia')\n",
        "    plt.title('Método Elbow para el número óptimo de clusters')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Graficar métricas adicionales\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Silhouette Score\n",
        "    axes[0].plot(k_range, silhouette_scores, marker='o', color='g', label='Silhouette Score')\n",
        "    axes[0].set_xlabel('Número de clusters (k)')\n",
        "    axes[0].set_ylabel('Silhouette Score')\n",
        "    axes[0].set_title('Silhouette Score (más alto es mejor)')\n",
        "    axes[0].grid(True)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Calinski-Harabasz Score\n",
        "    axes[1].plot(k_range, calinski_scores, marker='o', color='r', label='Calinski-Harabasz Score')\n",
        "    axes[1].set_xlabel('Número de clusters (k)')\n",
        "    axes[1].set_ylabel('Calinski-Harabasz Score')\n",
        "    axes[1].set_title('Calinski-Harabasz Score (más alto es mejor)')\n",
        "    axes[1].grid(True)\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Davies-Bouldin Score\n",
        "    axes[2].plot(k_range, davies_bouldin_scores, marker='o', color='purple', label='Davies-Bouldin Score')\n",
        "    axes[2].set_xlabel('Número de clusters (k)')\n",
        "    axes[2].set_ylabel('Davies-Bouldin Score')\n",
        "    axes[2].set_title('Davies-Bouldin Score (más bajo es mejor)')\n",
        "    axes[2].grid(True)\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "l7_Ilzrp6Dq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "def gmm_clustering_with_metrics(X, k_range=range(2, 9), title_prefix=\"\", model_name=\"\"):\n",
        "    # Convertir X a DataFrame si no lo es\n",
        "    if isinstance(X, np.ndarray):\n",
        "        X = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(X.shape[1])])\n",
        "\n",
        "    # Inicializar listas para almacenar las métricas\n",
        "    silhouette_scores = []\n",
        "    calinski_scores = []\n",
        "    davies_bouldin_scores = []\n",
        "\n",
        "    # Configurar subplots para una cuadrícula de 3x3 con un tamaño de 15x15 pulgadas\n",
        "    fig, axes = plt.subplots(7, 3, figsize=(20, 50))\n",
        "    axes = axes.flatten()  # Aplanar para iterar fácilmente\n",
        "\n",
        "    for i, k in enumerate(k_range):\n",
        "        gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "        gmm.fit(X)\n",
        "\n",
        "        # Obtener las etiquetas de clúster\n",
        "        labels = gmm.predict(X)\n",
        "\n",
        "        # Calcular y guardar las métricas\n",
        "        silhouette = silhouette_score(X, labels)\n",
        "        silhouette_scores.append(silhouette)\n",
        "\n",
        "        calinski = calinski_harabasz_score(X, labels)\n",
        "        calinski_scores.append(calinski)\n",
        "\n",
        "        davies_bouldin = davies_bouldin_score(X, labels)\n",
        "        davies_bouldin_scores.append(davies_bouldin)\n",
        "\n",
        "        # Guardar el modelo con nombre basado en las métricas\n",
        "        model_filename = f\"classification_model/{model_name}_k{k}_silh{silhouette:.2f}_calinski{calinski:.2f}_davies{davies_bouldin:.2f}.joblib\"\n",
        "        joblib.dump(gmm, model_filename)\n",
        "        print(f\"Model saved as: {model_filename}\")\n",
        "\n",
        "        # Visualizar clustering en 2D con métricas\n",
        "        axes[i].scatter(X.iloc[:, 0], X.iloc[:, 1], c=labels, cmap='viridis', s=30, alpha=0.6)\n",
        "        axes[i].scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n",
        "                        c='red', marker='x', s=100, linewidths=2, label='Centroids')\n",
        "        axes[i].set_title(f'{title_prefix}Clusters: {k}\\nSilhouette: {silhouette:.2f}\\n'\n",
        "                          f'Calinski: {calinski:.2f}\\nDavies-Bouldin: {davies_bouldin:.2f}')\n",
        "        axes[i].legend()\n",
        "\n",
        "    # Ocultar cualquier subplot adicional no utilizado\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Graficar métricas adicionales\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Silhouette Score\n",
        "    axes[0].plot(k_range, silhouette_scores, marker='o', color='g', label='Silhouette Score')\n",
        "    axes[0].set_xlabel('Número de clusters (k)')\n",
        "    axes[0].set_ylabel('Silhouette Score')\n",
        "    axes[0].set_title('Silhouette Score (más alto es mejor)')\n",
        "    axes[0].grid(True)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Calinski-Harabasz Score\n",
        "    axes[1].plot(k_range, calinski_scores, marker='o', color='r', label='Calinski-Harabasz Score')\n",
        "    axes[1].set_xlabel('Número de clusters (k)')\n",
        "    axes[1].set_ylabel('Calinski-Harabasz Score')\n",
        "    axes[1].set_title('Calinski-Harabasz Score (más alto es mejor)')\n",
        "    axes[1].grid(True)\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Davies-Bouldin Score\n",
        "    axes[2].plot(k_range, davies_bouldin_scores, marker='o', color='purple', label='Davies-Bouldin Score')\n",
        "    axes[2].set_xlabel('Número de clusters (k)')\n",
        "    axes[2].set_ylabel('Davies-Bouldin Score')\n",
        "    axes[2].set_title('Davies-Bouldin Score (más bajo es mejor)')\n",
        "    axes[2].grid(True)\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-CeUVBjK6JVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "def read_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X_latent = df.drop('original_column', axis=1).values\n",
        "    scaler = StandardScaler()\n",
        "    X_latent = scaler.fit_transform(X_latent)\n",
        "    return X_latent"
      ],
      "metadata": {
        "id": "NU8btarl6N0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_space_selection(path, k_range=range(2, 23), title_prefix=\"Kmeans\", model_name = \"\"):\n",
        "    X = read_dataset(path)\n",
        "    kmeans_clustering(X, k_range, title_prefix = 'Kmeans', model_name = 'Kmeans')\n",
        "    gmm_clustering_with_metrics(X, k_range, title_prefix = 'GMM', model_name = 'GMM')\n"
      ],
      "metadata": {
        "id": "Xsop_WNg6PHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a folder called classification_model\n",
        "\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(\"classification_model\"):\n",
        "    os.makedirs(\"classification_model\")\n",
        "    print(\"Directory 'classification_model' created successfully\")\n",
        "else:\n",
        "    print(\"Directory 'classification_model' already exists\")"
      ],
      "metadata": {
        "id": "_OEPvhRr6QXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf ##"
      ],
      "metadata": {
        "id": "5YbERynz6ZLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/isomap_ncomp20_nneigh40_error346.4654.csv'\n",
        "evaluate_space_selection(path)"
      ],
      "metadata": {
        "id": "7dQEppZu6eJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/autoencoder_typesparse_dim60_param1e-05_error0.8782.csv'\n",
        "evaluate_space_selection(path)"
      ],
      "metadata": {
        "id": "oHrMU9_a6f4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/umap_ncomp2_nneigh20_mindist0.7_metriccosine_error2006.8442.csv'\n",
        "evaluate_space_selection(path)"
      ],
      "metadata": {
        "id": "4OapjHAH6hjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf_2 ##"
      ],
      "metadata": {
        "id": "CfMPY6Lw6jjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/isomap_ncomp30_nneigh60_error288.6896.csv'\n",
        "evaluate_space_selection(path)"
      ],
      "metadata": {
        "id": "ZM_6cvPn6lvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/autoencoder_typesparse_dim60_param1e-05_error0.8737.csv'\n",
        "evaluate_space_selection(path)"
      ],
      "metadata": {
        "id": "hQXV2JcS6nld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/umap_ncomp2_nneigh20_mindist0.7_metriccosine_error2009.5221.csv'\n",
        "evaluate_space_selection(path)"
      ],
      "metadata": {
        "id": "sW9_vtsI6tHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduction Test for Umap #"
      ],
      "metadata": {
        "id": "P3KDa6zS6yH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from itertools import product\n",
        "import math\n",
        "\n",
        "def run_umap_with_tsne(X_scaled, X_query, param_grid, first_column_doc, first_column_query):\n",
        "    \"\"\"Run UMAP with different parameter combinations and plot reduced space with t-SNE.\"\"\"\n",
        "    results = []\n",
        "    min_error = float('inf')  # Track the minimum reconstruction error\n",
        "    best_idx = None           # Track the index of the best parameter combination\n",
        "\n",
        "    # Folder for saving CSVs\n",
        "    output_folder = \"Umap_2\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get the total number of parameter combinations\n",
        "    param_combinations = list(product(\n",
        "        param_grid['n_components'],\n",
        "        param_grid['n_neighbors'],\n",
        "        param_grid['min_dist'],\n",
        "        param_grid['metric']\n",
        "    ))\n",
        "\n",
        "    # Iterate through each combination of n_components, n_neighbors, and min_dist\n",
        "    for idx, (n_components, n_neighbors, min_dist, metric) in enumerate(param_combinations):\n",
        "        try:\n",
        "\n",
        "            # Validate parameters\n",
        "            if n_components > X_scaled.shape[1]:\n",
        "                print(f\"Skipping n_components={n_components}: exceeds feature dimensions.\")\n",
        "                continue\n",
        "            if n_neighbors >= X_scaled.shape[0]:\n",
        "                print(f\"Skipping n_neighbors={n_neighbors}: exceeds number of data points.\")\n",
        "                continue\n",
        "\n",
        "            # Run UMAP with the new parameter\n",
        "            umap_model = umap.UMAP(\n",
        "                n_neighbors=n_neighbors,\n",
        "                n_components=n_components,\n",
        "                min_dist=min_dist,\n",
        "                metric=metric,\n",
        "                random_state=42\n",
        "            )\n",
        "            umap_transformed_data = umap_model.fit_transform(X_scaled)\n",
        "\n",
        "            # Create DataFrame with first column\n",
        "            output_df = pd.DataFrame(umap_transformed_data)\n",
        "            output_df.insert(0, 'original_column', first_column_doc)\n",
        "\n",
        "            # Save the reduced space to a CSV file\n",
        "            file_name = f\"umap_ncomp{n_components}_nneigh{n_neighbors}_mindist{min_dist}_metric{metric}.csv\"\n",
        "            file_path = os.path.join(output_folder, file_name)\n",
        "            pd.DataFrame(output_df).to_csv(file_path, index=False)\n",
        "            print(f\"Saved Umap doc result to {file_path}\")\n",
        "\n",
        "            umap_transformed_data_query = umap_model.transform(X_query)\n",
        "\n",
        "            output_df_query = pd.DataFrame(umap_transformed_data_query)\n",
        "            output_df_query.insert(0, 'original_column', first_column_query)\n",
        "\n",
        "            file_name_query = f\"umap_query_ncomp{n_components}_nneigh{n_neighbors}_mindist{min_dist}_metric{metric}.csv\"\n",
        "            file_path_query = os.path.join(output_folder, file_name_query)\n",
        "            pd.DataFrame(output_df_query).to_csv(file_path_query, index=False)\n",
        "            print(f\"Saved Umap query result to {file_path_query}\")\n",
        "\n",
        "        except ValueError as ve:\n",
        "            print(f\"ValueError for n_components={n_components}, n_neighbors={n_neighbors}, min_dist={min_dist}: {str(ve)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error for n_components={n_components}, n_neighbors={n_neighbors}, min_dist={min_dist}: {str(e)}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "KC83N2dh7LKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "\n",
        "def cluster_documents_and_assign_queries(doc_csv, query_csv, n_clusters, output_csv):\n",
        "    \"\"\"\n",
        "    Clusters documents using K-Means and assigns queries to the closest cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - doc_csv: Path to the CSV file containing reduced dimensionality of documents.\n",
        "    - query_csv: Path to the CSV file containing reduced dimensionality of queries.\n",
        "    - n_clusters: Number of clusters for K-Means.\n",
        "    - output_csv: File path to save query-to-cluster assignment results.\n",
        "    \"\"\"\n",
        "    # Load the reduced dimensions for documents and queries\n",
        "    doc_data = pd.read_csv(doc_csv)\n",
        "    query_data = pd.read_csv(query_csv)\n",
        "\n",
        "    # Extract feature vectors and document/query names\n",
        "    document_vectors = doc_data[['0', '1']].values\n",
        "    document_names = doc_data['original_column'].values  # Document names\n",
        "\n",
        "    query_vectors = query_data[['0', '1']].values\n",
        "    query_names = query_data['original_column'].values  # Query names\n",
        "\n",
        "    # Apply K-Means clustering to documents\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(document_vectors)\n",
        "\n",
        "    # Get the cluster assignments for documents\n",
        "    doc_clusters = kmeans.labels_\n",
        "\n",
        "    # Assign queries to the closest cluster\n",
        "    query_cluster_indices, _ = pairwise_distances_argmin_min(query_vectors, kmeans.cluster_centers_)\n",
        "\n",
        "    # Prepare query-cluster matches\n",
        "    query_matches = []\n",
        "    for query_name, query_cluster_idx in zip(query_names, query_cluster_indices):\n",
        "        # Get documents belonging to the same cluster\n",
        "        matched_docs = [\n",
        "            document_names[idx] for idx, cluster in enumerate(doc_clusters) if cluster == query_cluster_idx\n",
        "        ]\n",
        "        query_matches.append({\n",
        "            'Query': query_name,\n",
        "            'Assigned_Cluster': query_cluster_idx,\n",
        "            'Matched_Documents': ', '.join(matched_docs)  # Join matched document names\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV\n",
        "    matches_df = pd.DataFrame(query_matches)\n",
        "    matches_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Query-to-cluster assignments saved to {output_csv}\")\n"
      ],
      "metadata": {
        "id": "8-7bTYPp7TKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf ##"
      ],
      "metadata": {
        "id": "tYgjEqaw7WS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_documents = 'tf_idf_documentos.csv'\n",
        "X_doc, first_column_doc = read_and_prepare_data(file_path_documents, 'standard')\n",
        "\n",
        "file_path_queries = 'queries_vector.csv'\n",
        "X_queries, first_column_queries = read_and_prepare_data(file_path_queries, 'standard')"
      ],
      "metadata": {
        "id": "js9dIJft7ZaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_components': [2],\n",
        "    'n_neighbors': [20],\n",
        "    'min_dist': [0.7],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "\n",
        "run_umap_with_tsne(X_doc, X_queries, param_grid, first_column_doc, first_column_queries)"
      ],
      "metadata": {
        "id": "j4pjmzxN7a8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "doc_csv = \"/content/Umap/umap_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"   # CSV with reduced dimensions of documents\n",
        "query_csv = \"/content/Umap/umap_query_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"   # CSV with reduced dimensions of queries\n",
        "output_csv = \"query_cluster_matches.csv\"  # Output CSV with matches\n",
        "n_clusters = 6  # Number of neighbors for KNN\n",
        "\n",
        "cluster_documents_and_assign_queries(doc_csv, query_csv, n_clusters, output_csv)\n"
      ],
      "metadata": {
        "id": "fv7wreLR7cgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the queries CSV\n",
        "query_csv = \"/content/Umap/umap_query_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "query_data = pd.read_csv(query_csv)\n",
        "\n",
        "document_csv = \"/content/Umap/umap_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "document_data = pd.read_csv(document_csv)\n"
      ],
      "metadata": {
        "id": "rql3twrd7mWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity_for_selected_documents(documents_df, query_vector, selected_documents):\n",
        "    # Filter the DataFrame to include only the selected documents\n",
        "    selected_df = documents_df[documents_df['original_column'].isin(selected_documents)]\n",
        "\n",
        "    # Extract the vectors of the selected documents\n",
        "    selected_vectors = selected_df.iloc[:, 1:].values  # Excludes 'original_column'\n",
        "\n",
        "    # Compute cosine similarity between each selected document vector and the query vector\n",
        "    similarities = cosine_similarity(selected_vectors, query_vector.reshape(1, -1)).flatten()\n",
        "\n",
        "    # Create a result DataFrame with document names and their similarity scores\n",
        "    results = pd.DataFrame({\n",
        "        'document': selected_df['original_column'].values,\n",
        "        'similarity': similarities\n",
        "    })\n",
        "\n",
        "    # Sort the results by similarity in descending order\n",
        "    results = results.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "PduStJEq7oG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_similar_documents_manhattan(query_vector, document_data, selected_documents):\n",
        "\n",
        "    # Filter the document data to include only the selected documents\n",
        "    filtered_data = document_data[document_data['original_column'].isin(selected_documents)]\n",
        "\n",
        "    # Extract the document names and their corresponding vectors\n",
        "    document_names = filtered_data['original_column'].values\n",
        "    document_vectors = filtered_data[['0', '1']].values  # Adjust if there are more dimensions\n",
        "\n",
        "    # Compute Manhattan distances between the query vector and all selected document vectors\n",
        "    distances = np.sum(np.abs(document_vectors - query_vector), axis=1)\n",
        "\n",
        "    # Combine document names with their distances and sort by distance\n",
        "    results = list(zip(document_names, distances))\n",
        "    results.sort(key=lambda x: x[1])  # Sort by distance (ascending)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "mkqhyS1h7pd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_similar_documents_pearson(query_vector, document_data, selected_documents):\n",
        "    # Filter the document data to include only the selected documents\n",
        "    filtered_data = document_data[document_data['original_column'].isin(selected_documents)]\n",
        "\n",
        "    # Extract the document names and their corresponding vectors\n",
        "    document_names = filtered_data['original_column'].values\n",
        "    document_vectors = filtered_data[['0', '1']].values  # Adjust if there are more dimensions\n",
        "\n",
        "    # Calculate Pearson correlation between the query vector and all document vectors\n",
        "    correlations = []\n",
        "    for doc_vector in document_vectors:\n",
        "        # Pearson correlation: np.corrcoef returns a correlation matrix; take the off-diagonal element\n",
        "        correlation = np.corrcoef(query_vector, doc_vector)[0, 1]\n",
        "        correlations.append(correlation)\n",
        "\n",
        "    # Combine document names with their correlations and sort by correlation (descending)\n",
        "    results = list(zip(document_names, correlations))\n",
        "    results.sort(key=lambda x: x[1], reverse=True)  # Sort by correlation (descending)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "-MbXdd6O7qpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine ###"
      ],
      "metadata": {
        "id": "YJG0Npk27sI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Umap/umap_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "query_csv = \"/content/Umap/umap_query_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)\n",
        "\n",
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[0].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"32_Emprendimiento.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\",\n",
        "    \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"63_Servicio_y_Protocolo.txt.txt\",\n",
        "    \"67_Innovacion_Culinaria.txt.txt\",\n",
        "    \"47_Conceptos_y_Tecnicas_1.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"56_Carniceria.txt.txt\",\n",
        "    \"64_Practica_Culinaria_2.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"54_Practica_Culinaria_1.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"69_Administracion_de_Eventos.txt.txt\",\n",
        "    \"61_Alta_Cocina_Francesa.txt.txt\",\n",
        "    \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"57_Alta_Cocina_Ecuatoriana.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"49_Panaderia.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "hWfFy_Vy7tUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[1].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"95_Fisiologia_+Lab.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"96_Evolucion.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\"\n",
        "]\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "JfaZHtvB7vWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[2].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"95_Fisiologia_+Lab.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"96_Evolucion.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\"\n",
        "]\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "VioTvLwh7w_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[3].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"105_Marketing_Digital.txt.txt\",\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"101_Analisis_de_Datos.txt.txt\",\n",
        "    \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"36_Aprendizaje_Automatico.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\",\n",
        "    \"55_Administracion_de_A_&_B.txt.txt\",\n",
        "    \"42_Seguridad_Informatica.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"106_Negociacion.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"43_Mineria_de_Datos.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"48_Principios_de_Administracion.txt.txt\",\n",
        "    \"52_Contabilidad_Empresarial.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"59_Introduccion_al_Marketing_HSP.txt.txt\",\n",
        "    \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"51_Introduccion_a_la_Hospitalidad.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\"\n",
        "]\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "qGdPdxaCVfsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[4].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"34_Inteligencia_Artificial.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"28_Probabilidad_y_Estadistica_+Ej.txt.txt\",\n",
        "    \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\",\n",
        "    \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"76_Estadistica_para_CCSS.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"156_Inferencia_Estadistica.txt.txt\",\n",
        "    \"168_Analisis_Real.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "TWUOR7XY8nwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan ##"
      ],
      "metadata": {
        "id": "Z_G0F-_g8pT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Umap/umap_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "query_csv = \"/content/Umap/umap_query_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)\n",
        "\n",
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[0].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"32_Emprendimiento.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\",\n",
        "    \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"63_Servicio_y_Protocolo.txt.txt\",\n",
        "    \"67_Innovacion_Culinaria.txt.txt\",\n",
        "    \"47_Conceptos_y_Tecnicas_1.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"56_Carniceria.txt.txt\",\n",
        "    \"64_Practica_Culinaria_2.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"54_Practica_Culinaria_1.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"69_Administracion_de_Eventos.txt.txt\",\n",
        "    \"61_Alta_Cocina_Francesa.txt.txt\",\n",
        "    \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"57_Alta_Cocina_Ecuatoriana.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"49_Panaderia.txt.txt\"\n",
        "]\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "CZ-mMYiT89-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[1].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"95_Fisiologia_+Lab.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"96_Evolucion.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "nSulnelI9AF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[2].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"95_Fisiologia_+Lab.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"96_Evolucion.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\"\n",
        "]\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "jgk3mY7N9Bm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[3].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"105_Marketing_Digital.txt.txt\",\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"101_Analisis_de_Datos.txt.txt\",\n",
        "    \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"36_Aprendizaje_Automatico.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\",\n",
        "    \"55_Administracion_de_A_&_B.txt.txt\",\n",
        "    \"42_Seguridad_Informatica.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"106_Negociacion.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"43_Mineria_de_Datos.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"48_Principios_de_Administracion.txt.txt\",\n",
        "    \"52_Contabilidad_Empresarial.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"59_Introduccion_al_Marketing_HSP.txt.txt\",\n",
        "    \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"51_Introduccion_a_la_Hospitalidad.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "A05UROlu9DBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[4].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"34_Inteligencia_Artificial.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"28_Probabilidad_y_Estadistica_+Ej.txt.txt\",\n",
        "    \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\",\n",
        "    \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"76_Estadistica_para_CCSS.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"156_Inferencia_Estadistica.txt.txt\",\n",
        "    \"168_Analisis_Real.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "P-nPSVS79FPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf_2 ##"
      ],
      "metadata": {
        "id": "rjm1IG_r9HqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_documents = 'tf_idf_documentos_2.csv'\n",
        "X_doc, first_column_doc = read_and_prepare_data(file_path_documents, 'standard')\n",
        "\n",
        "file_path_queries = 'queries_vector_2.csv'\n",
        "X_queries, first_column_queries = read_and_prepare_data(file_path_queries, 'standard')"
      ],
      "metadata": {
        "id": "YRkljGVT99Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_components': [2],\n",
        "    'n_neighbors': [20],\n",
        "    'min_dist': [0.7],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "\n",
        "run_umap_with_tsne(X_doc, X_queries, param_grid, first_column_doc, first_column_queries)"
      ],
      "metadata": {
        "id": "Qjt5aqV59-MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "doc_csv = \"/content/Umap_2/umap_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"   # CSV with reduced dimensions of documents\n",
        "query_csv = \"/content/Umap_2/umap_query_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"   # CSV with reduced dimensions of queries\n",
        "output_csv = \"query_cluster_matches_2.csv\"  # Output CSV with matches\n",
        "n_clusters = 3  # Number of neighbors for KNN\n",
        "\n",
        "cluster_documents_and_assign_queries(doc_csv, query_csv, n_clusters, output_csv)"
      ],
      "metadata": {
        "id": "_r0_-nT29_ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine ###"
      ],
      "metadata": {
        "id": "scRc6YDS9_5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Umap_2/umap_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "query_csv = \"/content/Umap_2/umap_query_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)"
      ],
      "metadata": {
        "id": "s097kAPm-cj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[0].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"105_Marketing_Digital.txt.txt\",\n",
        "    \"32_Emprendimiento.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\",\n",
        "    \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"63_Servicio_y_Protocolo.txt.txt\",\n",
        "    \"67_Innovacion_Culinaria.txt.txt\",\n",
        "    \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"47_Conceptos_y_Tecnicas_1.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"56_Carniceria.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"25_Cultura_Gastronomica.txt.txt\",\n",
        "    \"55_Administracion_de_A_&_B.txt.txt\",\n",
        "    \"64_Practica_Culinaria_2.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"54_Practica_Culinaria_1.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"106_Negociacion.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"93_Practica_4.txt.txt\",\n",
        "    \"69_Administracion_de_Eventos.txt.txt\",\n",
        "    \"61_Alta_Cocina_Francesa.txt.txt\",\n",
        "    \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\",\n",
        "    \"57_Alta_Cocina_Ecuatoriana.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"75_Planificacion_y_Evaluacion_1.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"48_Principios_de_Administracion.txt.txt\",\n",
        "    \"90_Practica_3.txt.txt\",\n",
        "    \"52_Contabilidad_Empresarial.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"95_Fisiologia_+Lab.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"87_Practica_2.txt.txt\",\n",
        "    \"96_Evolucion.txt.txt\",\n",
        "    \"59_Introduccion_al_Marketing_HSP.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"49_Panaderia.txt.txt\",\n",
        "    \"71_Identidad_Culinaria.txt.txt\",\n",
        "    \"51_Introduccion_a_la_Hospitalidad.txt.txt\"\n",
        "]\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "C-M4woFh-eRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[1].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\"184_Ergonomia.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\",\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"85_Arte_y_Educacion.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"78_Planificacion_y_Evaluacion_2.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"60_Coloquios_Gastr.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"91_Inclusion_y_Diversidad.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"81_Practica_1.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"84_Ensenanza_de_Ciencias_Sociales.txt.txt\",\n",
        "    \"132_Enfasis_1.txt.txt\",\n",
        "    \"20_Ingles_Nivel_6.txt.txt\",\n",
        "    \"134_Enfasis_2.txt.txt\",\n",
        "    \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"88_Ensenanza_de_Ciencias.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"73_Teorias_del_Aprendizaje.txt.txt\",\n",
        "    \"74_Desarrollo__NiÃ±o_y_Adolescente.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"128_Taller_de_Arte_1.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"19_Ingles_Nivel_5.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "KRt0Fkyu-gm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[2].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\"184_Ergonomia.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\",\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"85_Arte_y_Educacion.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"78_Planificacion_y_Evaluacion_2.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"60_Coloquios_Gastr.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"91_Inclusion_y_Diversidad.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"81_Practica_1.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"84_Ensenanza_de_Ciencias_Sociales.txt.txt\",\n",
        "    \"132_Enfasis_1.txt.txt\",\n",
        "    \"20_Ingles_Nivel_6.txt.txt\",\n",
        "    \"134_Enfasis_2.txt.txt\",\n",
        "    \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"88_Ensenanza_de_Ciencias.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"73_Teorias_del_Aprendizaje.txt.txt\",\n",
        "    \"74_Desarrollo__NiÃ±o_y_Adolescente.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"128_Taller_de_Arte_1.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"19_Ingles_Nivel_5.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        "\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "Busmvmve-hKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[3].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"34_Inteligencia_Artificial.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\",\n",
        "    \"101_Analisis_de_Datos.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\",\n",
        "    \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"36_Aprendizaje_Automatico.txt.txt\",\n",
        "    \"28_Probabilidad_y_Estadistica_+Ej.txt.txt\",\n",
        "    \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"42_Seguridad_Informatica.txt.txt\",\n",
        "    \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"43_Mineria_de_Datos.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"76_Estadistica_para_CCSS.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"156_Inferencia_Estadistica.txt.txt\",\n",
        "    \"168_Analisis_Real.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\",\n",
        "    \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        "\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)\n"
      ],
      "metadata": {
        "id": "02-d99mA-ikU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[4].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\"184_Ergonomia.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\",\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"85_Arte_y_Educacion.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"78_Planificacion_y_Evaluacion_2.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"60_Coloquios_Gastr.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"91_Inclusion_y_Diversidad.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"81_Practica_1.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"84_Ensenanza_de_Ciencias_Sociales.txt.txt\",\n",
        "    \"132_Enfasis_1.txt.txt\",\n",
        "    \"20_Ingles_Nivel_6.txt.txt\",\n",
        "    \"134_Enfasis_2.txt.txt\",\n",
        "    \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"88_Ensenanza_de_Ciencias.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"73_Teorias_del_Aprendizaje.txt.txt\",\n",
        "    \"74_Desarrollo__NiÃ±o_y_Adolescente.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"128_Taller_de_Arte_1.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"19_Ingles_Nivel_5.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        "\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)\n"
      ],
      "metadata": {
        "id": "oPhCn7l4-jys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan ###"
      ],
      "metadata": {
        "id": "0mqx_CHG-lWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Umap_2/umap_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "query_csv = \"/content/Umap_2/umap_query_ncomp2_nneigh20_mindist0.7_metriccosine.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)"
      ],
      "metadata": {
        "id": "v7L0ee3--x-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[0].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"105_Marketing_Digital.txt.txt\",\n",
        "    \"32_Emprendimiento.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\",\n",
        "    \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"63_Servicio_y_Protocolo.txt.txt\",\n",
        "    \"67_Innovacion_Culinaria.txt.txt\",\n",
        "    \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"47_Conceptos_y_Tecnicas_1.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"56_Carniceria.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"25_Cultura_Gastronomica.txt.txt\",\n",
        "    \"55_Administracion_de_A_&_B.txt.txt\",\n",
        "    \"64_Practica_Culinaria_2.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"54_Practica_Culinaria_1.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"106_Negociacion.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"93_Practica_4.txt.txt\",\n",
        "    \"69_Administracion_de_Eventos.txt.txt\",\n",
        "    \"61_Alta_Cocina_Francesa.txt.txt\",\n",
        "    \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\",\n",
        "    \"57_Alta_Cocina_Ecuatoriana.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"75_Planificacion_y_Evaluacion_1.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"48_Principios_de_Administracion.txt.txt\",\n",
        "    \"90_Practica_3.txt.txt\",\n",
        "    \"52_Contabilidad_Empresarial.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"95_Fisiologia_+Lab.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"87_Practica_2.txt.txt\",\n",
        "    \"96_Evolucion.txt.txt\",\n",
        "    \"59_Introduccion_al_Marketing_HSP.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"49_Panaderia.txt.txt\",\n",
        "    \"71_Identidad_Culinaria.txt.txt\",\n",
        "    \"51_Introduccion_a_la_Hospitalidad.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "iciz0zvD-z9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[1].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\"184_Ergonomia.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\",\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"85_Arte_y_Educacion.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"78_Planificacion_y_Evaluacion_2.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"60_Coloquios_Gastr.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"91_Inclusion_y_Diversidad.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"81_Practica_1.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"84_Ensenanza_de_Ciencias_Sociales.txt.txt\",\n",
        "    \"132_Enfasis_1.txt.txt\",\n",
        "    \"20_Ingles_Nivel_6.txt.txt\",\n",
        "    \"134_Enfasis_2.txt.txt\",\n",
        "    \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"88_Ensenanza_de_Ciencias.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"73_Teorias_del_Aprendizaje.txt.txt\",\n",
        "    \"74_Desarrollo__NiÃ±o_y_Adolescente.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"128_Taller_de_Arte_1.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"19_Ingles_Nivel_5.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "wAuKl9uD-0dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[2].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\"184_Ergonomia.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\",\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"85_Arte_y_Educacion.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"78_Planificacion_y_Evaluacion_2.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"60_Coloquios_Gastr.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"91_Inclusion_y_Diversidad.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"81_Practica_1.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"84_Ensenanza_de_Ciencias_Sociales.txt.txt\",\n",
        "    \"132_Enfasis_1.txt.txt\",\n",
        "    \"20_Ingles_Nivel_6.txt.txt\",\n",
        "    \"134_Enfasis_2.txt.txt\",\n",
        "    \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"88_Ensenanza_de_Ciencias.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"73_Teorias_del_Aprendizaje.txt.txt\",\n",
        "    \"74_Desarrollo__NiÃ±o_y_Adolescente.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"128_Taller_de_Arte_1.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"19_Ingles_Nivel_5.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        "\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "qpZQzrsA-11I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[3].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"34_Inteligencia_Artificial.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\",\n",
        "    \"101_Analisis_de_Datos.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\",\n",
        "    \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"36_Aprendizaje_Automatico.txt.txt\",\n",
        "    \"28_Probabilidad_y_Estadistica_+Ej.txt.txt\",\n",
        "    \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"42_Seguridad_Informatica.txt.txt\",\n",
        "    \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"43_Mineria_de_Datos.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"76_Estadistica_para_CCSS.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"156_Inferencia_Estadistica.txt.txt\",\n",
        "    \"168_Analisis_Real.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\",\n",
        "    \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        "# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "EonrUwUj-34w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[4].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\"184_Ergonomia.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\",\n",
        "    \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\",\n",
        "    \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"178_Sonido.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"85_Arte_y_Educacion.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"78_Planificacion_y_Evaluacion_2.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"60_Coloquios_Gastr.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"91_Inclusion_y_Diversidad.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"81_Practica_1.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"84_Ensenanza_de_Ciencias_Sociales.txt.txt\",\n",
        "    \"132_Enfasis_1.txt.txt\",\n",
        "    \"20_Ingles_Nivel_6.txt.txt\",\n",
        "    \"134_Enfasis_2.txt.txt\",\n",
        "    \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"88_Ensenanza_de_Ciencias.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"73_Teorias_del_Aprendizaje.txt.txt\",\n",
        "    \"74_Desarrollo__NiÃ±o_y_Adolescente.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"122_Fotografia_1.txt.txt\",\n",
        "    \"128_Taller_de_Arte_1.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"19_Ingles_Nivel_5.txt.txt\",\n",
        "    \"177_Cinematografia.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"6_Ingles_Nivel_1.txt.txt\",\n",
        "    \"179_Storytelling.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "YRfEMAeH-5mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduction Test for Isomap #"
      ],
      "metadata": {
        "id": "pGRTF2zW-8pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.manifold import Isomap\n",
        "from itertools import product\n",
        "\n",
        "def run_isomap_with_tsne(X_scaled, X_query, param_grid, first_column_doc, first_column_query):\n",
        "    \"\"\"Run Isomap with different parameter combinations and save the reduced space.\"\"\"\n",
        "    results = []\n",
        "    min_error = float('inf')  # Track the minimum reconstruction error\n",
        "    best_idx = None           # Track the index of the best parameter combination\n",
        "\n",
        "    # Folder for saving CSVs\n",
        "    output_folder = \"Isomap_Results\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Convert inputs to numpy arrays if they are DataFrames\n",
        "    if isinstance(X_scaled, pd.DataFrame):\n",
        "        X_scaled = X_scaled.to_numpy()\n",
        "    if isinstance(X_query, pd.DataFrame):\n",
        "        X_query = X_query.to_numpy()\n",
        "\n",
        "    # Get the total number of parameter combinations\n",
        "    param_combinations = list(product(\n",
        "        param_grid['n_components'],\n",
        "        param_grid['n_neighbors']\n",
        "    ))\n",
        "\n",
        "    # Iterate through each combination of n_components and n_neighbors\n",
        "    for idx, (n_components, n_neighbors) in enumerate(param_combinations):\n",
        "        try:\n",
        "            # Validate parameters\n",
        "            if n_components > X_scaled.shape[1]:\n",
        "                print(f\"Skipping n_components={n_components}: exceeds feature dimensions.\")\n",
        "                continue\n",
        "            if n_neighbors >= X_scaled.shape[0]:\n",
        "                print(f\"Skipping n_neighbors={n_neighbors}: exceeds number of data points.\")\n",
        "                continue\n",
        "\n",
        "            # Run Isomap with the new parameters\n",
        "            isomap_model = Isomap(\n",
        "                n_neighbors=n_neighbors,\n",
        "                n_components=n_components\n",
        "            )\n",
        "            isomap_transformed_data = isomap_model.fit_transform(X_scaled)\n",
        "\n",
        "            # Create DataFrame with first column\n",
        "            output_df = pd.DataFrame(isomap_transformed_data)\n",
        "            output_df.insert(0, 'original_column', first_column_doc)\n",
        "\n",
        "            # Save the reduced space to a CSV file\n",
        "            file_name = f\"isomap_ncomp{n_components}_nneigh{n_neighbors}.csv\"\n",
        "            file_path = os.path.join(output_folder, file_name)\n",
        "            pd.DataFrame(output_df).to_csv(file_path, index=False)\n",
        "            print(f\"Saved Isomap doc result to {file_path}\")\n",
        "\n",
        "            # Transform the query data using the same Isomap model\n",
        "            isomap_transformed_data_query = isomap_model.transform(X_query)\n",
        "\n",
        "            output_df_query = pd.DataFrame(isomap_transformed_data_query)\n",
        "            output_df_query.insert(0, 'original_column', first_column_query)\n",
        "\n",
        "            file_name_query = f\"isomap_query_ncomp{n_components}_nneigh{n_neighbors}.csv\"\n",
        "            file_path_query = os.path.join(output_folder, file_name_query)\n",
        "            pd.DataFrame(output_df_query).to_csv(file_path_query, index=False)\n",
        "            print(f\"Saved Isomap query result to {file_path_query}\")\n",
        "\n",
        "        except ValueError as ve:\n",
        "            print(f\"ValueError for n_components={n_components}, n_neighbors={n_neighbors}: {str(ve)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error for n_components={n_components}, n_neighbors={n_neighbors}: {str(e)}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Zk-I7jHZ_EY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf ##"
      ],
      "metadata": {
        "id": "sQj-B-uA_qKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_documents = 'tf_idf_documentos.csv'\n",
        "X_doc, first_column_doc = read_and_prepare_data(file_path_documents, 'standard')\n",
        "\n",
        "file_path_queries = 'queries_vector.csv'\n",
        "X_queries, first_column_queries = read_and_prepare_data(file_path_queries, 'standard')"
      ],
      "metadata": {
        "id": "ll5guFCU_9qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_components': [20],\n",
        "    'n_neighbors': [40]\n",
        "}\n",
        "\n",
        "run_isomap_with_tsne(X_doc, X_queries, param_grid, first_column_doc, first_column_queries)"
      ],
      "metadata": {
        "id": "zOZTgelR_-2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "doc_csv = \"/content/Isomap_Results/isomap_ncomp20_nneigh40.csv\"   # CSV with reduced dimensions of documents\n",
        "query_csv = \"/content/Isomap_Results/isomap_query_ncomp20_nneigh40.csv\"   # CSV with reduced dimensions of queries\n",
        "output_csv = \"query_cluster_matches_isomap.csv\"  # Output CSV with matches\n",
        "n_clusters = 22  # Number of neighbors for KNN\n",
        "\n",
        "cluster_documents_and_assign_queries(doc_csv, query_csv, n_clusters, output_csv)\n"
      ],
      "metadata": {
        "id": "SuvGA1B2AAQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the queries CSV\n",
        "query_csv = \"/content/Isomap_Results/isomap_ncomp20_nneigh40.csv\"\n",
        "query_data = pd.read_csv(query_csv)\n",
        "\n",
        "document_csv = \"/content/Isomap_Results/isomap_query_ncomp20_nneigh40.csv\"\n",
        "document_data = pd.read_csv(document_csv)\n"
      ],
      "metadata": {
        "id": "rSp88sZZAEl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity_for_selected_documents(documents_df, query_vector, selected_documents):\n",
        "    # Filter the DataFrame to include only the selected documents\n",
        "    selected_df = documents_df[documents_df['original_column'].isin(selected_documents)]\n",
        "\n",
        "    # Extract the vectors of the selected documents\n",
        "    selected_vectors = selected_df.iloc[:, 1:].values  # Excludes 'original_column'\n",
        "\n",
        "    # Compute cosine similarity between each selected document vector and the query vector\n",
        "    similarities = cosine_similarity(selected_vectors, query_vector.reshape(1, -1)).flatten()\n",
        "\n",
        "    # Create a result DataFrame with document names and their similarity scores\n",
        "    results = pd.DataFrame({\n",
        "        'document': selected_df['original_column'].values,\n",
        "        'similarity': similarities\n",
        "    })\n",
        "\n",
        "    # Sort the results by similarity in descending order\n",
        "    results = results.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "CpvgEufRAFuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_similar_documents_manhattan(query_vector, document_data, selected_documents):\n",
        "\n",
        "    # Filter the document data to include only the selected documents\n",
        "    filtered_data = document_data[document_data['original_column'].isin(selected_documents)]\n",
        "\n",
        "    # Extract the document names and their corresponding vectors\n",
        "    document_names = filtered_data['original_column'].values\n",
        "    document_vectors = filtered_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].values  # Adjust if there are more dimensions\n",
        "\n",
        "    # Compute Manhattan distances between the query vector and all selected document vectors\n",
        "    distances = np.sum(np.abs(document_vectors - query_vector), axis=1)\n",
        "\n",
        "    # Combine document names with their distances and sort by distance\n",
        "    results = list(zip(document_names, distances))\n",
        "    results.sort(key=lambda x: x[1])  # Sort by distance (ascending)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "SEdO9NRkAGbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine ###"
      ],
      "metadata": {
        "id": "yei3btn9AIkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Isomap_Results/isomap_ncomp20_nneigh40.csv\"\n",
        "query_csv = \"/content/Isomap_Results/isomap_query_ncomp20_nneigh40.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)\n",
        "\n",
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[0].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        "\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "2-xFAVrMAKv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[1].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "nYH1hILLAL9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[2].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "] # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "MPLKoo2zAMgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[3].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "xTizxFx_AN04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[4].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "NcJkyfPAARZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan ###"
      ],
      "metadata": {
        "id": "lVnnaEVnATVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Isomap_Results/isomap_ncomp20_nneigh40.csv\"\n",
        "query_csv = \"/content/Isomap_Results/isomap_query_ncomp20_nneigh40.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)\n",
        "\n",
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[0].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "tyHwDRvAAVhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[1].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "bwo63HeWAW3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[2].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "o8wVgB7kAYkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[3].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "3obJevfZAaEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19']].iloc[4].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\",\n",
        "    \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"35_Base_de_Datos.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\",\n",
        "    \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"151_Ecuaciones_Diferenciales.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"4_Quimica_General_1_+Lab_Ej.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\",\n",
        "    \"17_Fisica_para_Ingenieria_1_+Lab_Ej.txt.txt\",\n",
        "    \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "    \"80_Ensenanza_de_Lenguaje.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\",\n",
        "    \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\",\n",
        "    \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\",\n",
        "    \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"23_Fisica_para_Ingenieria_2_+Lab_Ej.txt.txt\",\n",
        "    \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\",\n",
        "    \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"31_Organizacion_de_Computadores.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\",\n",
        "    \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\",\n",
        "    \"41_Coloquios_ING.txt.txt\",\n",
        "    \"111_Proyectos_Empresariales.txt.txt\",\n",
        "    \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"70_Gerencia_Financiera_HSP.txt.txt\",\n",
        "    \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\",\n",
        "    \"159_Combinatoria_y_Grafos.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\",\n",
        "    \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\",\n",
        "    \"37_Redes_+Lab.txt.txt\",\n",
        "    \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "]  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "8GbCCQxkAdRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf_2 ##"
      ],
      "metadata": {
        "id": "P1kYBTXrAgjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_documents = 'tf_idf_documentos_2.csv'\n",
        "X_doc, first_column_doc = read_and_prepare_data(file_path_documents, 'standard')\n",
        "\n",
        "file_path_queries = 'queries_vector_2.csv'\n",
        "X_queries, first_column_queries = read_and_prepare_data(file_path_queries, 'standard')"
      ],
      "metadata": {
        "id": "xLfpm33BAqGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_components': [30],\n",
        "    'n_neighbors': [60]\n",
        "}\n",
        "\n",
        "run_isomap_with_tsne(X_doc, X_queries, param_grid, first_column_doc, first_column_queries)"
      ],
      "metadata": {
        "id": "xMvxXTMtArQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def cluster_documents_and_assign_queries(doc_csv, query_csv, n_clusters, output_csv):\n",
        "    \"\"\"\n",
        "    Clusters documents using K-Means and assigns queries to the closest cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - doc_csv: Path to the CSV file containing reduced dimensionality of documents.\n",
        "    - query_csv: Path to the CSV file containing reduced dimensionality of queries.\n",
        "    - n_clusters: Number of clusters for K-Means.\n",
        "    - output_csv: File path to save query-to-cluster assignment results.\n",
        "    \"\"\"\n",
        "    # Load the reduced dimensions for documents and queries\n",
        "    doc_data = pd.read_csv(doc_csv)\n",
        "    query_data = pd.read_csv(query_csv)\n",
        "\n",
        "    # Extract feature vectors and document/query names\n",
        "    document_vectors = doc_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].values\n",
        "\n",
        "    document_names = doc_data['original_column'].values  # Document names\n",
        "\n",
        "    query_vectors = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].values\n",
        "    query_names = query_data['original_column'].values  # Query names\n",
        "\n",
        "    # Apply K-Means clustering to documents\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(document_vectors)\n",
        "\n",
        "    # Get the cluster assignments for documents\n",
        "    doc_clusters = kmeans.labels_\n",
        "\n",
        "    # Assign queries to the closest cluster\n",
        "    query_cluster_indices, _ = pairwise_distances_argmin_min(query_vectors, kmeans.cluster_centers_)\n",
        "\n",
        "    # Prepare query-cluster matches\n",
        "    query_matches = []\n",
        "    for query_name, query_cluster_idx in zip(query_names, query_cluster_indices):\n",
        "        # Get documents belonging to the same cluster\n",
        "        matched_docs = [\n",
        "            document_names[idx] for idx, cluster in enumerate(doc_clusters) if cluster == query_cluster_idx\n",
        "        ]\n",
        "        query_matches.append({\n",
        "            'Query': query_name,\n",
        "            'Assigned_Cluster': query_cluster_idx,\n",
        "            'Matched_Documents': ', '.join(matched_docs)  # Join matched document names\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV\n",
        "    matches_df = pd.DataFrame(query_matches)\n",
        "    matches_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Query-to-cluster assignments saved to {output_csv}\")\n"
      ],
      "metadata": {
        "id": "R-W_uXySAs8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "doc_csv = \"/content/Isomap_Results/isomap_ncomp30_nneigh60.csv\"   # CSV with reduced dimensions of documents\n",
        "query_csv = \"/content/Isomap_Results/isomap_query_ncomp30_nneigh60.csv\"   # CSV with reduced dimensions of queries\n",
        "output_csv = \"query_cluster_matches_2.csv\"  # Output CSV with matches\n",
        "n_clusters = 22  # Number of neighbors for KNN\n",
        "\n",
        "cluster_documents_and_assign_queries(doc_csv, query_csv, n_clusters, output_csv)"
      ],
      "metadata": {
        "id": "s6eTePEeAvgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine ###"
      ],
      "metadata": {
        "id": "dY_gqRS-AwTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_similar_documents_manhattan(query_vector, document_data, selected_documents):\n",
        "\n",
        "    # Filter the document data to include only the selected documents\n",
        "    filtered_data = document_data[document_data['original_column'].isin(selected_documents)]\n",
        "\n",
        "    # Extract the document names and their corresponding vectors\n",
        "    document_names = filtered_data['original_column'].values\n",
        "    document_vectors = filtered_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].values  # Adjust if there are more dimensions\n",
        "\n",
        "    # Compute Manhattan distances between the query vector and all selected document vectors\n",
        "    distances = np.sum(np.abs(document_vectors - query_vector), axis=1)\n",
        "\n",
        "    # Combine document names with their distances and sort by distance\n",
        "    results = list(zip(document_names, distances))\n",
        "    results.sort(key=lambda x: x[1])  # Sort by distance (ascending)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Ht3riNItA1a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Isomap_Results/isomap_ncomp30_nneigh60.csv\"\n",
        "query_csv = \"/content/Isomap_Results/isomap_query_ncomp30_nneigh60.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)"
      ],
      "metadata": {
        "id": "Gv77dYkvA21J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].iloc[0].values\n",
        "\n",
        "selected_documents = [\n",
        "    \"184_Ergonomia.txt.txt\", \"105_Marketing_Digital.txt.txt\", \"32_Emprendimiento.txt.txt\",\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\", \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\", \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\", \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\", \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\", \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\", \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\", \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\", \"47_Conceptos_y_Tecnicas_1.txt.txt\", \"35_Base_de_Datos.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\", \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\", \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\", \"151_Ecuaciones_Diferenciales.txt.txt\", \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\", \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\", \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\", \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\", \"4_Quimica_General_1_+Lab_Ej.txt.txt\", \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\", \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\", \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\", \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\", \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\", \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\", \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\", \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\", \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\", \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\", \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\", \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\", \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\", \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\", \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\", \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\", \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\", \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\", \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\", \"81_Practica_1.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\", \"75_Planificacion_y_Evaluacion_1.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\", \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\", \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\", \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\", \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\", \"132_Enfasis_1.txt.txt\", \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\", \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\", \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\", \"134_Enfasis_2.txt.txt\", \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\", \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\", \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\", \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\", \"111_Proyectos_Empresariales.txt.txt\", \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\", \"168_Analisis_Real.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\", \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\", \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\", \"128_Taller_de_Arte_1.txt.txt\", \"49_Panaderia.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\", \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\", \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\", \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\", \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\", \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\", \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "f_sfPZ8vA4B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].iloc[1].values\n",
        "\n",
        "selected_documents = [\n",
        "    \"184_Ergonomia.txt.txt\", \"105_Marketing_Digital.txt.txt\", \"32_Emprendimiento.txt.txt\",\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\", \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\", \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\", \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\", \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\", \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\", \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\", \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\", \"47_Conceptos_y_Tecnicas_1.txt.txt\", \"35_Base_de_Datos.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\", \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\", \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\", \"151_Ecuaciones_Diferenciales.txt.txt\", \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\", \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\", \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\", \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\", \"4_Quimica_General_1_+Lab_Ej.txt.txt\", \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\", \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\", \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\", \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\", \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\", \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\", \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\", \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\", \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\", \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\", \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\", \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\", \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\", \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\", \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\", \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\", \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\", \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\", \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\", \"81_Practica_1.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\", \"75_Planificacion_y_Evaluacion_1.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\", \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\", \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\", \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\", \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\", \"132_Enfasis_1.txt.txt\", \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\", \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\", \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\", \"134_Enfasis_2.txt.txt\", \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\", \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\", \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\", \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\", \"111_Proyectos_Empresariales.txt.txt\", \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\", \"168_Analisis_Real.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\", \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\", \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\", \"128_Taller_de_Arte_1.txt.txt\", \"49_Panaderia.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\", \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\", \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\", \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\", \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\", \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\", \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "M63gj7PHA5im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].iloc[2].values\n",
        "\n",
        "selected_documents = [\n",
        "    \"184_Ergonomia.txt.txt\", \"105_Marketing_Digital.txt.txt\", \"32_Emprendimiento.txt.txt\",\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\", \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\", \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\", \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\", \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\", \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\", \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\", \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\", \"47_Conceptos_y_Tecnicas_1.txt.txt\", \"35_Base_de_Datos.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\", \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\", \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\", \"151_Ecuaciones_Diferenciales.txt.txt\", \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\", \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\", \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\", \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\", \"4_Quimica_General_1_+Lab_Ej.txt.txt\", \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\", \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\", \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\", \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\", \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\", \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\", \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\", \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\", \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\", \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\", \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\", \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\", \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\", \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\", \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\", \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\", \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\", \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\", \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\", \"81_Practica_1.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\", \"75_Planificacion_y_Evaluacion_1.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\", \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\", \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\", \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\", \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\", \"132_Enfasis_1.txt.txt\", \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\", \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\", \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\", \"134_Enfasis_2.txt.txt\", \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\", \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\", \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\", \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\", \"111_Proyectos_Empresariales.txt.txt\", \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\", \"168_Analisis_Real.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\", \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\", \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\", \"128_Taller_de_Arte_1.txt.txt\", \"49_Panaderia.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\", \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\", \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\", \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\", \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\", \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\", \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "B391m5ouA69-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].iloc[3].values\n",
        "\n",
        "selected_documents = [\n",
        "    \"184_Ergonomia.txt.txt\", \"105_Marketing_Digital.txt.txt\", \"32_Emprendimiento.txt.txt\",\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\", \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\", \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\", \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\", \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\", \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\", \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\", \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\", \"47_Conceptos_y_Tecnicas_1.txt.txt\", \"35_Base_de_Datos.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\", \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\", \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\", \"151_Ecuaciones_Diferenciales.txt.txt\", \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\", \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\", \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\", \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\", \"4_Quimica_General_1_+Lab_Ej.txt.txt\", \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\", \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\", \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\", \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\", \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\", \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\", \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\", \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\", \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\", \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\", \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\", \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\", \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\", \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\", \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\", \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\", \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\", \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\", \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\", \"81_Practica_1.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\", \"75_Planificacion_y_Evaluacion_1.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\", \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\", \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\", \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\", \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\", \"132_Enfasis_1.txt.txt\", \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\", \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\", \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\", \"134_Enfasis_2.txt.txt\", \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\", \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\", \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\", \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\", \"111_Proyectos_Empresariales.txt.txt\", \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\", \"168_Analisis_Real.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\", \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\", \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\", \"128_Taller_de_Arte_1.txt.txt\", \"49_Panaderia.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\", \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\", \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\", \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\", \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\", \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\", \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "cv5Hd7vnA8R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                                 '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
        "                                '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']].iloc[4].values\n",
        "\n",
        "selected_documents = [\n",
        "    \"184_Ergonomia.txt.txt\", \"105_Marketing_Digital.txt.txt\", \"32_Emprendimiento.txt.txt\",\n",
        "    \"185_Ingenieria_de_la_Calidad_+_Lab.txt.txt\", \"24_Aprendizaje_y_Servicio_PASEC.txt.txt\",\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\", \"11_Ser_y_Cosmos.txt.txt\",\n",
        "    \"89_Ensenanza_Integrada_de_CITIAM.txt.txt\", \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"26_Electronica_Basica_+Lab.txt.txt\", \"7_Ingles_Nivel_2.txt.txt\",\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\", \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\", \"183_Procesos,_Metodos_y_Estandares.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\", \"116_Analisis_Estrategico_ADM.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\", \"47_Conceptos_y_Tecnicas_1.txt.txt\", \"35_Base_de_Datos.txt.txt\",\n",
        "    \"119_Composicion_Visual_1.txt.txt\", \"79_Coloquios_EDU.txt.txt\",\n",
        "    \"130_Arte_y_Contexto_Social.txt.txt\", \"44_Aplicaciones_Distribuidas.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\", \"151_Ecuaciones_Diferenciales.txt.txt\", \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\", \"100_Principios_de_Seguros.txt.txt\",\n",
        "    \"72_Fundamentos_de_la_Educacion.txt.txt\", \"58_Introduccion_a_la_Biologia_+Ej.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\", \"181_Inv._de_Operaciones_1_+Lab.txt.txt\",\n",
        "    \"186_Sistemas_Lean.txt.txt\", \"4_Quimica_General_1_+Lab_Ej.txt.txt\", \"138_Enfasis_3.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\", \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"99_Gerencia_de_Costos.txt.txt\", \"127_Laboratorio_de_Creacion_2.txt.txt\",\n",
        "    \"176_Lenguaje_Visual_y_Montaje.txt.txt\", \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"30_Programacion_Avanzada_de_Apps.txt.txt\", \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"50_Nutricion_Humana_+Lab.txt.txt\", \"182_Control_de_Produccion.txt.txt\",\n",
        "    \"118_Fundamentos_de_las_Artes.txt.txt\", \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\", \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\", \"83_Ensenanza_de_Matematicas.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\", \"140_Arte_y_Educacion___Curaduria.txt.txt\",\n",
        "    \"1_Escritura_Academica.txt.txt\", \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\", \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\", \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\", \"86_Bilingualism.txt.txt\",\n",
        "    \"180_DiseÃ±o_de_Produccion.txt.txt\", \"97_Principios_de_Marketing.txt.txt\",\n",
        "    \"109_Investigacion_de_Mercados.txt.txt\", \"21_Programacion_de_Apps.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\", \"14_Programacion_Avanzada_en_C++.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\", \"13_Ingles_Nivel_4.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\", \"142_Produccion_&_Exhibicion.txt.txt\",\n",
        "    \"152_Calculo_para_Ciencias_2.txt.txt\", \"81_Practica_1.txt.txt\",\n",
        "    \"16_Introduccion_a_la_Economia.txt.txt\", \"75_Planificacion_y_Evaluacion_1.txt.txt\",\n",
        "    \"8_Programacion_en_C++_+Ej.txt.txt\", \"143_Programacion_Para_DiseÃ±o_1.txt.txt\",\n",
        "    \"108_Economia_y_Negocios.txt.txt\", \"98_Estadistica_Empresarial_+Lab.txt.txt\",\n",
        "    \"82_Metodologias_de_Ensenanza.txt.txt\", \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"121_Dibujo_para_Arte_y_Diseno.txt.txt\", \"131_Taller_de_Investigacion.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\", \"132_Enfasis_1.txt.txt\", \"107_Operaciones_+Lab.txt.txt\",\n",
        "    \"145_Programacion_Para_DiseÃ±o_3.txt.txt\", \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"112_Gestion_del_Talento.txt.txt\", \"77_Neurociencia_y_Educacion.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\", \"134_Enfasis_2.txt.txt\", \"148_Calculo_para_Ciencias_1.txt.txt\",\n",
        "    \"46_Matematica_Empresarial_+Ej.txt.txt\", \"147_Modelado_3D_1.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\", \"39_Proyectos__Gerencia_y_Analisis.txt.txt\",\n",
        "    \"27_Estructuras_de_Datos.txt.txt\", \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"173_Performance.txt.txt\", \"111_Proyectos_Empresariales.txt.txt\", \"157_Analisis_Numerico.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\", \"168_Analisis_Real.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\", \"38_Sistemas_Operativos.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\", \"136_Laboratorio_de_Creacion_3.txt.txt\",\n",
        "    \"5_Cosmos.txt.txt\", \"128_Taller_de_Arte_1.txt.txt\", \"49_Panaderia.txt.txt\",\n",
        "    \"144_Programacion_Para_DiseÃ±o_2.txt.txt\", \"103_Coloquios_adm.txt.txt\",\n",
        "    \"120_Laboratorio_de_Creacion_1.txt.txt\", \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "    \"133_Taller_de_Arte_2.txt.txt\", \"146_Juegos_y_Narrativa.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\", \"139_Taller_de_Arte_3.txt.txt\",\n",
        "    \"33_DiseÃ±o_de_Sistemas.txt.txt\", \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\", \"158_Teoria_de_Numeros.txt.txt\"\n",
        "]\n",
        " # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = calculate_similarity_for_selected_documents(document_data, query_vector, selected_documents)\n",
        "\n",
        "print(similarity_results)"
      ],
      "metadata": {
        "id": "vnIjrlJ-A9oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan ###"
      ],
      "metadata": {
        "id": "MFhK-z1JBAR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "document_csv = \"/content/Isomap_Results/isomap_ncomp30_nneigh60.csv\"\n",
        "query_csv = \"/content/Isomap_Results/isomap_query_ncomp30_nneigh60.csv\"\n",
        "\n",
        "# Load data\n",
        "document_data = pd.read_csv(document_csv)\n",
        "query_data = pd.read_csv(query_csv)"
      ],
      "metadata": {
        "id": "3KBPWZv5BGc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use the first query vector\n",
        "query_vector = query_data[['0', '1']].iloc[0].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "                      \"119_Composicion_Visual_1.txt.txt\",\n",
        "                      \"178_Sonido.txt.txt\",\n",
        "                      \"176_Lenguaje_Visual_y_Montaje.txt.txt\",\n",
        "                      \"117_Herramientas_Digitales_1.txt.txt\",\n",
        "                      \"123_Fundamentos_de_Escultura.txt.txt\",\n",
        "                      \"180_DiseÃ±o_de_Produccion.txt.txt\",\n",
        "                      \"122_Fotografia_1.txt.txt\",\n",
        "                      \"175_Lenguaje_del_Cine.txt.txt\",\n",
        "                      \"177_Cinematografia.txt.txt\",\n",
        "                      \"179_Storytelling.txt.txt\",\n",
        "                      \"146_Juegos_y_Narrativa.txt.txt\"\n",
        "                  ]   # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "aGOb415lBHfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[1].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"184_Ergonomia.txt.txt\",\n",
        "    \"105_Marketing_Digital.txt.txt\",\n",
        "    \"10_Autoconocimiento.txt.txt\",\n",
        "    \"102_Principios_de_Finanzas.txt.txt\",\n",
        "    \"135_Gestion_y_Produccion_Cultural.txt.txt\",\n",
        "    \"60_Coloquios_Gastr.txt.txt\",\n",
        "    \"91_Inclusion_y_Diversidad.txt.txt\",\n",
        "    \"2_Taller_de_Ing._Cs._Computacion.txt.txt\",\n",
        "    \"93_Practica_4.txt.txt\",\n",
        "    \"12_Ingles_Nivel_3.txt.txt\",\n",
        "    \"45_Proyecto_Integrador_CMP.txt.txt\",\n",
        "    \"94_Zoologia_+Lab.txt.txt\",\n",
        "    \"95_Fisiologia_+Lab.txt.txt\",\n",
        "    \"174_Proyecto_final_en_Danza.txt.txt\",\n",
        "    \"29_Teoria_de_la_Computacion.txt.txt\",\n",
        "    \"74_Desarrollo__NiÃ±o_y_Adolescente.txt.txt\",\n",
        "    \"92_Proyecto_Integrador_EDU.txt.txt\",\n",
        "    \"40_Practica_Pre-Profesional_PASEM.txt.txt\",\n",
        "    \"103_Coloquios_adm.txt.txt\"\n",
        "]# Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "mhVdYmG0BJlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[2].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"65_Alta_Cocina_Mundial.txt.txt\",\n",
        "    \"110_Creatividad_Empresarial.txt.txt\",\n",
        "    \"67_Innovacion_Culinaria.txt.txt\",\n",
        "    \"47_Conceptos_y_Tecnicas_1.txt.txt\",\n",
        "    \"62_Pasteleria.txt.txt\",\n",
        "    \"25_Cultura_Gastronomica.txt.txt\",\n",
        "    \"64_Practica_Culinaria_2.txt.txt\",\n",
        "    \"66_Reposteria_y_Chocolateria.txt.txt\",\n",
        "    \"53_Conceptos_y_Tecnicas_2_+PRA.txt.txt\",\n",
        "    \"54_Practica_Culinaria_1.txt.txt\",\n",
        "    \"115_Tributacion_y_Entorno_Legal.txt.txt\",\n",
        "    \"61_Alta_Cocina_Francesa.txt.txt\",\n",
        "    \"57_Alta_Cocina_Ecuatoriana.txt.txt\",\n",
        "    \"68_Enologia_y_Cocteleria.txt.txt\",\n",
        "    \"113_Negocios_Internacionales.txt.txt\",\n",
        "    \"114_Innovacion_y_Sustentabilidad.txt.txt\",\n",
        "    \"104_Finanzas_Corporativas.txt.txt\",\n",
        "    \"49_Panaderia.txt.txt\",\n",
        "    \"71_Identidad_Culinaria.txt.txt\"\n",
        "]\n",
        "  # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "u7xgecJXBLSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[3].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"171_Barra_para_danza_contemporanea.txt.txt\",\n",
        "    \"129_Coloquios_ART.txt.txt\",\n",
        "    \"125_Nuevos_Medios.txt.txt\",\n",
        "    \"138_Enfasis_3.txt.txt\",\n",
        "    \"126_Arte_Contemporaneo.txt.txt\",\n",
        "    \"169_Improvisacion.txt.txt\",\n",
        "    \"132_Enfasis_1.txt.txt\",\n",
        "    \"134_Enfasis_2.txt.txt\",\n",
        "    \"173_Performance.txt.txt\",\n",
        "    \"172_Composicion.txt.txt\",\n",
        "    \"170_Danza_Moderna_2.txt.txt\",\n",
        "    \"141_Temas_en_Comunicacion_y_Arte.txt.txt\",\n",
        "    \"124_Teoria_Critica_1__Arte_&_Media.txt.txt\"\n",
        "] # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "78Qrt8jFBMfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = query_data[['0', '1']].iloc[4].values\n",
        "\n",
        "# Select specific document names\n",
        "selected_documents = [\n",
        "    \"3_Calculo_Diferencial_+_Ej.txt.txt\",\n",
        "    \"155_Logica_y_Teoria_de_Conjuntos.txt.txt\",\n",
        "    \"161_Algebra_Lineal_2.txt.txt\",\n",
        "    \"18_Calculo_Vectorial.txt.txt\",\n",
        "    \"162_Algebra_Abstracta_1.txt.txt\",\n",
        "    \"164_Analisis_Funcional.txt.txt\",\n",
        "    \"153_Introduccion_a_Probabilidades.txt.txt\",\n",
        "    \"150_Variable_Compleja.txt.txt\",\n",
        "    \"165_Algebra_Abstracta_2.txt.txt\",\n",
        "    \"15_Matematicas_Discretas.txt.txt\",\n",
        "    \"163_Geometria_Diferencial.txt.txt\",\n",
        "    \"22_Algebra_Lineal_1_+Ej.txt.txt\",\n",
        "    \"137_Matematicas_Cotidianas.txt.txt\",\n",
        "    \"154_Fundamentos_de_Geometria.txt.txt\",\n",
        "    \"149_Teoria_de_Grupos.txt.txt\",\n",
        "    \"160_Ecuaciones_Diferenciales_Parciales.txt.txt\",\n",
        "    \"9_Calculo_Integral_+_Ej.txt.txt\",\n",
        "    \"166_Topologia_1.txt.txt\",\n",
        "    \"168_Analisis_Real.txt.txt\",\n",
        "    \"167_Topologia_2.txt.txt\",\n",
        "    \"158_Teoria_de_Numeros.txt.txt\"\n",
        "] # Replace with the actual document names\n",
        "\n",
        "# Calculate similarity for selected documents\n",
        "similarity_results = get_similar_documents_manhattan(query_vector, document_data, selected_documents)\n",
        "\n",
        "for i in range(len(similarity_results)):\n",
        "    print(f\"Document: {similarity_results[i][0]}, Distance: {similarity_results[i][1]}\")"
      ],
      "metadata": {
        "id": "K9FJs5rCBNzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test for normal tf_idf #"
      ],
      "metadata": {
        "id": "OQT0coJpBPna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf ##"
      ],
      "metadata": {
        "id": "YZJmZORiCtTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read and save two csv files as df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "documentos = pd.read_csv(\"/content/tf_idf_documentos.csv\") #replace file1.csv with your actual file name\n",
        "queries = pd.read_csv(\"/content/queries_vector.csv\") #replace file2.csv with your actual file name"
      ],
      "metadata": {
        "id": "p6GSPgJRBvlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine ###"
      ],
      "metadata": {
        "id": "s8g8Weu6CxWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_cosine_similarity(query_vector, document_vectors):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between a single query vector and all document vectors.\n",
        "\n",
        "    Args:\n",
        "        query_vector (list or array): A single query vector.\n",
        "        document_vectors (array): Matrix of document vectors.\n",
        "\n",
        "    Returns:\n",
        "        list: Cosine similarity scores between the query and each document.\n",
        "    \"\"\"\n",
        "    query_vector = query_vector.reshape(1, -1)  # Reshape for compatibility\n",
        "    similarities = cosine_similarity(query_vector, document_vectors)\n",
        "    return similarities[0]  # Extract the scores as a flat list"
      ],
      "metadata": {
        "id": "eUk4QEL7Bws8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract document vectors (numeric columns only)\n",
        "document_vectors = documentos.iloc[:, 1:].values  # Adjust indices as needed\n",
        "\n",
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 0  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "EW78NMQxBxyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 1  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "ZQmbXzJFBzDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 2  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "XuNP-q7FB0iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 3  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "VAbNzGjvB2dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 4  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "TgJDYe-BB3xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan ###"
      ],
      "metadata": {
        "id": "jKNUAxf2EvW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "\n",
        "def compute_manhattan_distance(query_vector, document_vectors):\n",
        "    \"\"\"\n",
        "    Computes Manhattan distance between a single query vector and all document vectors.\n",
        "\n",
        "    Args:\n",
        "        query_vector (list or array): A single query vector.\n",
        "        document_vectors (array): Matrix of document vectors.\n",
        "\n",
        "    Returns:\n",
        "        list: Manhattan distances between the query and each document.\n",
        "    \"\"\"\n",
        "    query_vector = query_vector.reshape(1, -1)  # Reshape for compatibility\n",
        "    distances = manhattan_distances(query_vector, document_vectors)\n",
        "    return distances[0]  # Extract the distances as a flat list"
      ],
      "metadata": {
        "id": "sGjDZSAMEwoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 0  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "wnYYXMRYEy0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 1  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "OXKkj5rQE0rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 2  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "1KC_Z6K5E2Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 3  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "AawUZu8TE2rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 4  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "SPYsYZ0xE3Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf_idf_2 ##"
      ],
      "metadata": {
        "id": "78qKGllsE6jG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine ###"
      ],
      "metadata": {
        "id": "MTTg_SrEFMPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read and save two csv files as df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "documentos = pd.read_csv(\"/content/tf_idf_documentos_2.csv\") #replace file1.csv with your actual file name\n",
        "queries = pd.read_csv(\"/content/queries_vector_2.csv\") #replace file2.csv with your actual file name"
      ],
      "metadata": {
        "id": "0s_zS0Z8FCBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract document vectors (numeric columns only)\n",
        "document_vectors = documentos.iloc[:, 1:].values  # Adjust indices as needed\n",
        "\n",
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 0  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "waUAvCjSFDUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 1  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "jlCVXPZ2FEhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 2  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "8IJELkXYFFDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 3  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "lgAvm3-pFFak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 4  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_cosine_similarity(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Similarity Score': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nlargest(5, 'Similarity Score')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "DU5VRW2uFJF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan ###"
      ],
      "metadata": {
        "id": "6V7i77fcFOra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 0  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "OuO88JYOFQd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 1  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "kbmgx79lFXLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 2  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "hIJJioRqFZDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 3  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "fiXXW4TiFaii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute similarity for one query row\n",
        "query_row_index = 4  # Index of the query to process\n",
        "query_vector = queries.iloc[query_row_index, 1:].values  # Extract a single query vector\n",
        "similarity_scores = compute_manhattan_distance(query_vector, document_vectors)\n",
        "\n",
        "# Map the results back to document identifiers\n",
        "document_ids = documentos.iloc[:, 0]  # Assuming the first column contains document identifiers\n",
        "results = pd.DataFrame({\n",
        "    'Document ID': document_ids,\n",
        "    'Distance': similarity_scores\n",
        "})\n",
        "\n",
        "# Display the top 5 matches\n",
        "top_matches = results.nsmallest(5, 'Distance')\n",
        "print(top_matches)"
      ],
      "metadata": {
        "id": "kLV6N0sEFcSB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}